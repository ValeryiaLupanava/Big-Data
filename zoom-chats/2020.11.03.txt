19:03:08	 От  Dmitry Vorob : ансамбли
19:04:50	 От  Семён Шафронов : А где кнопка?
19:05:06	 От  Dmitry Novikov : Где участники
19:05:26	 От  Семён Шафронов : Нашел. Спасибо
19:11:21	 От  Катерина Кучерявенко : лики
19:11:33	 От  Марина Капуш : Много пустых значений
19:13:07	 От  Марина Капуш : Что значит лики?)
19:19:00	 От  Максим Кобзев : Не очень уложилась в голове практическая техника кросс-валидации (теорию вроде понимаю, а руки не понимают)… Какие есть базовые методики в питоне?
19:19:39	 От  Александр Бобр : Есть ли задачи для которых обычные модели лучше ансамблей?
19:20:28	 От  Dmitry Pyshkin : Петр, а можете концептуально рассказать, как бы Вы делали наш проект?) Нужна Вара помощь.
19:20:36	 От  Victoria : линейная зависимость?
19:20:42	 От  Марина Капуш : + к кросс валидации вопрос: когда мы сдвигаем обучающий датасет мы же заново обучаем модель, по какому принципу получаем итоговую?
19:34:31	 От  Константин Иванченко : что это пиликает?
19:46:58	 От  Семён Шафронов : Выбор с возвращением.
19:51:04	 От  ermakovpetr : —————
19:51:05	 От  Viktor Gurov : 187
19:51:10	 От  Semyon Bochkaryov : 140
19:51:10	 От  andreychubin : 130
19:51:13	 От  Максим Кобзев : 235.9
19:51:13	 От  Семён Шафронов : 300
19:51:13	 От  Алексей : 215
19:51:14	 От  Sergei : 210
19:51:14	 От  Марина Капуш : 50
19:51:15	 От  Dmitry Novikov : 220
19:51:17	 От  Alexander Alexandrov : 234
19:51:21	 От  Maria : 120
19:51:23	 От  Константин Иванченко : 196
19:51:24	 От  Егор Макрушин : 200
19:51:24	 От  Victoria : 300
19:51:25	 От  Михаил Новиков : 140
19:51:26	 От  Tatiana : 150
19:51:27	 От  Александр Бобр : 350
19:51:31	 От  Alex Softwarer : 184
19:51:37	 От  Ekaterina Medvedeva : 200
19:51:41	 От  Dmitry Vorob : 300
19:51:42	 От  Ruslan Tskhovrebadze : 200
19:51:43	 От  Павел Минькин : 400
19:51:47	 От  Victoria : 202
19:52:27	 От  Dmitry Pyshkin : 140
19:53:03	 От  Михаил Новиков : загуглил после :)
19:54:50	 От  andreychubin : Теперь всё стало ясно, RF больше не используем))
19:55:24	 От  Victoria : Деревья же не видят друг друга, мы тут все таки подвержены мнению, которое уже видим
19:55:46	 От  Viktor Gurov : )))
19:55:50	 От  Viktor Gurov : я перепутал 
19:55:51	 От  Марина Капуш : Я перепутала с пизанской башней))
19:55:53	 От  Viktor Gurov : с другой башней
19:56:39	 От  andreychubin : У нас видимо гиперпараметры были неправильные
19:57:11	 От  Ruslan Tskhovrebadze : В ходе проекта возник вопрос про one-hot encoding: если у нас очень много уникальных значений категориальных фичей (100 тыс и больше) - какую структуру лучше использовать для их формирования и подачи в модель? Всякие Sparse - структуры, или есть более подходящее решение? Или использовать one-hot encoding в таких случаях - не лучший путь?
19:58:25	 От  Семён Шафронов : А какие методы хорошо работают с разреженными данными?
20:00:12	 От  Семён Шафронов : TF IDF
20:00:16	 От  Semyon Bochkaryov : Векторизация?
20:00:30	 От  Dmitry Vorob : Чет не помню
20:00:44	 От  Алексей : Проходили мимо.
20:00:49	 От  Dmitry Pyshkin : Давайте еще раз вспомним про каждый…
20:00:50	 От  Катерина Кучерявенко : да, мельком было
20:01:32	 От  Dmitry Pyshkin : Вспомнил, спасибо
20:06:21	 От  Алексей : Пентаграммы...
20:06:34	 От  Алексей : Не брать...
20:06:45	 От  Максим Кобзев : ))))))
20:13:12	 От  Dmitry Vorob : классиф
20:13:16	 От  Максим Кобзев : clf
20:13:16	 От  Семён Шафронов : классификация.
20:13:20	 От  Константин Иванченко : классификация
20:18:38	 От  Константин Иванченко : да
20:22:28	 От  Александр Бобр : Что мы предсказываем, ещё раз?
20:23:47	 От  Семён Шафронов : Предсказываем - позитивный текст или негативный
20:24:06	 От  Александр Бобр : Спс
20:26:14	 От  Dmitry Vorob : Модели с таким кол-вом столбцов очень долго обучаются, если текстов много?
20:27:03	 От  Dmitry Vorob : А можно еще раз на пальцах вот про разрежаю данные, как оно понимает где эти единицы
20:27:09	 От  Alexander Alexandrov : А если нам надо еще какие-то признаки передать, помимо разреженной матрицы, то какие способы?
20:27:55	 От  Dmitry Vorob : Понятно, но координаты дешевле хранить
20:28:35	 От  Семён Шафронов : XGB (не в пакете SKLearn) требует преобразовывать все train в разреженную матрицу (Dmatrix). И обрабатывает быстрее
20:28:54	 От  Ruslan Tskhovrebadze : А можно соединять разреженные матрицы друг с другом? И как лучше соединять с «обычными» матрицами  - к сожалению часто падает по памяти
20:30:45	 От  Dmitry Vorob : В продолжение вопрос к доп фичам. Если у нас каждое значение в столбце разное для каждого элемента, то в целом даже если координаты добавятся, то это все равно дешевле по памяти будет за счет экономиии на другой части фдатасета. Верно понял?
20:33:27	 От  Alexander Alexandrov : Там три числа и хранится - два это координаты + значение
20:34:58	 От  Семён Шафронов : Круто! Интересно, по строчкам?
20:36:02	 От  Dmitry Vorob : Дано. Большая таблица где большое кол-во нулей. Фичи - слова в текстах. Выгодно хранить в разряженных данных, так как нулей много. Допустим добавляем еще кастомную фичу, например “время публикации” (грубо для примера). Время будет отличаться в каждой строке. Если мы делаем sparse, то с т.з. памяти хранить наш кастомный столбец будет дороже, чем в случае незаряженной матрицы). Но за счет того что нулей все равно очень много остается, этот подход выгоднее остается. Верно?
20:37:47	 От  Семён Шафронов : Класс! Именно с такой проблемой я и столкнулся при выполнении проекта
20:38:09	 От  Александр Бобр : А я на лабе 4ой
20:39:52	 От  Михаил Новиков : действительно пора
20:41:00	 От  Victoria : А новые слова из теста он просто игнорирует?
20:41:05	 От  Семён Шафронов : Супер. А я вручную колонки недостающие добавлял)
20:41:13	 От  Михаил Новиков : А если  появились новые слова? на тесте он их дропнет
20:42:08	 От  Максим Кобзев : Это как использовать шаблон из фичей (пустую матрицу) и потом его заполнить тестом?
20:43:16	 От  Семён Шафронов : А для модели неважно название колонок, а только количество? 
20:45:06	 От  Dmitry Vorob : Таймер на экран? )
20:45:15	 От  Александр Бобр : А вопросы можно будет задать?
20:45:23	 От  Александр Бобр : Ок
20:45:35	 От  Семён Шафронов : Спасибо. Про transform - очень полезно)
21:00:30	 От  Dmitry Vorob : Все, понял. А зачем тогда вообще фит_транфсорм нужен7
21:00:52	 От  Dmitry Vorob : понял
21:00:57	 От  Dmitry Vorob : спасибо
21:02:13	 От  Alexander Alexandrov : А имеет смысл делать фитр_трансформ на трейн+тесте. У нас же все равно слова, которые есть только в тесте в модели не будут учитываться, потому что их в трейне нет.
21:03:39	 От  Максим Кобзев : Почаще показывай розмарин ))))
21:03:40	 От  Александр Бобр : Я только ща понял мем :(
21:04:46	 От  Dmitry Vorob : Я вообще мем не понял )
21:04:56	 От  Viktor Gurov : нужен спойлер
21:05:26	 От  Михаил Новиков : расскажете про multilabel multioutput модели ? когда несколько классов и колонок в таргете одновременно
21:07:44	 От  Dmitry Novikov : 5%
21:09:39	 От  Dmitry Novikov : 0.5
21:13:38	 От  Александр Бобр : Прикольно
21:14:04	 От  Dmitry Vorob : То есть линейная и логистическая это одно и то же?
21:14:31	 От  Maria : а вариант с таргетом, у которого не 2 категории, а больше, будет?
21:15:31	 От  Семён Шафронов : Логистическая хорошо работает со спарс матрицами, а XGB - плохо. Но XGB в основе лежит binary:logistic (логистическая регрессия). Тогда что применять?
21:15:32	 От  Михаил Новиков : он сделает 1у категорию :)
21:18:08	 От  Семён Шафронов : binary:logistic: logistic regression for binary classification, output probability  https://xgboost.readthedocs.io/en/latest/parameter.html
21:24:19	 От  Семён Шафронов : Пётр, возможно ли как-то графически показать, как действует регуляризация? В общих словах - это понятно, но как и когда применять - сложно.
21:27:41	 От  Семён Шафронов : А использовать регуляризации рандомным перебором и оценкой метрики или интеллектуально?
21:29:49	 От  Tatiana : если применять вместе one hot encoding и регуляризацию, то линейные модели дают адекватный результат?
21:34:29	 От  Семён Шафронов : А если мы не обучаем в одном ноутбуке, а загружаем модель из Pickle? Тогда нужно будет и OHE запоминать и загружать?
21:36:57	 От  Алексей : игнор
21:37:40	 От  Денис Маркин : А как теперь с остальными фичами соединить?
21:38:18	 От  Семён Шафронов : Т.е чтобы не было перемешивания, нужно выполнить fittransform (train) и transform (test)?
21:40:08	 От  Михаил Новиков : feature union как объединяет данные,
21:40:10	 От  Михаил Новиков : ?
21:40:16	 От  Михаил Новиков : да
21:42:37	 От  Alexander Alexandrov : А получается лучше все преобразования через пайплайн делать и его пиклить?
21:43:16	 От  Alexander Alexandrov : А как просто колонку преобразовать через пайплайн?
21:43:27	 От  Максим Кобзев : Если упрощать, то получается весь NLP крутится вокруг fit_transform (сделал шаблон и обучил на нем модель) и transform (заполнил шаблон) и запихнуть это в прогноз?
21:44:01	 От  Михаил Новиков : Александр FunctionTransformer() в пайплайнет
21:44:38	 От  Александр Бобр :  Вылетело
21:44:46	 От  Alexander Alexandrov : Михаил, спасибо!
21:44:48	 От  Александр Бобр : Стоп-слова напомните, что такое?
21:45:38	 От  Александр Бобр : Ага, понял, спасибо!
21:45:53	 От  Максим Кобзев : Да да, если опустить саму обработку и подготовку текста… то для готового набора данных: cv.fit_transform+model.fit -> cv.transform+model.predict
21:48:00	 От  Максим Кобзев : Ага! Спасибо!
21:48:04	 От  andreychubin : Можно ли в Pipeline использовать свои функции?
21:48:47	 От  andreychubin : Да, но я ничего не понял
21:49:22	 От  Михаил Новиков : FunctionTransformer(custom func object)
21:49:40	 От  Михаил Новиков : labda x: len(x)
21:50:49	 От  ermakovpetr : https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65
21:51:16	 От  andreychubin : спасибо, теперь понятнее
21:58:37	 От  Максим Кобзев : Ага
21:58:41	 От  Максим Кобзев : Огонёк
21:58:42	 От  Семён Шафронов : И это ответ руководителю на вопрос: почему эффективность модели у тебя скачет каждый день?
21:59:31	 От  Максим Кобзев : А можно еще пару слов про GridSearch?
22:01:23	 От  Семён Шафронов : Ок)
22:08:59	 От  Dmitry Vorob : А кол-во ядер тут нельзя задать?
22:10:19	 От  Dmitry Novikov : Круто
22:10:25	 От  Семён Шафронов : Интереснее выгружать таблицу с матрицей параметров и смотреть, как меняется эффективность в зависимости от параметров.
22:11:16	 От  Dmitry Novikov : Помогают программировать :)
22:11:32	 От  Ruslan Tskhovrebadze : CatBoost ))
22:11:38	 От  Dmitry Novikov : :))))
22:11:44	 От  Семён Шафронов : :)
22:13:57	 От  Alexander Alexandrov : а еще вопрос - у нас в проекте можно делать предикт не для всей тестовой выборки. А в этом есть смысл и как лучше всего это делать? Есть какие-то критерии?
22:14:19	 От  Семён Шафронов : CV
22:14:44	 От  ermakovpetr : https://github.com/ermakovpetr/npl_for_students/blob/master/ml_on_text_bow.ipynb
22:18:04	 От  Семён Шафронов : в XGB objective - вместо binary:logistic - multi:softprob
22:18:35	 От  Семён Шафронов : Пётр, в начале лекций ты рассказал про дообучение моделей? Как это можно  делать (именно дообучать, а не заново обучать на новой выборке)? 
22:19:46	 От  Семён Шафронов : Спасибо!
22:21:04	 От  Dmitry Novikov : Спасибо большое :)
22:21:04	 От  Alexander Alexandrov : Спасибо!
22:21:05	 От  Dmitry Vorob : спасибо
22:21:07	 От  Дмитрий Мошинец : спасибо
22:21:14	 От  Victoria : Спасибо!
22:21:22	 От  Dmitry Vorob : Хочу отдельно на какой-нить твой курс )
22:21:27	 От  Maria : Спасибо!!!
22:21:40	 От  Tatiana : спасибо!
22:22:20	 От  Максим Кобзев : Пётр, спасибо большое! Все четко, полезно и настраивает!
22:22:23	 От  Алексей : Пока, спасибо большое.
22:22:27	 От  Катерина Кучерявенко : спасибо))все было оч круто
22:22:50	 От  allburn : Пётр - топ! спасибо
22:29:04	 От  Михаил Новиков : Круто Спасибо!!!!
