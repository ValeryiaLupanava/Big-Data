11:02:04	 От  Семён Шафронов : Всем привет!
11:04:19	 От  Semyon Bochkaryov : Казалось бы, при чем тут Иннополис? :-)
11:17:03	 От  Dmitry Novikov : Точность вырастет до 97,3%
11:18:16	 От  Nikita Syrov : 0.79
11:18:25	 От  andreychubin : 146%, нет?
11:19:08	 От  Павел Минькин : 97%
11:31:12	 От  Семён Шафронов : Переобучение?
11:32:11	 От  Семён Шафронов : Понял. Спасибо.
11:36:01	 От  Victoria : Почему использование менее скоррелированных моделей уменьшают смещение?
11:40:13	 От  Dmitry Novikov : Но bias у Bagging'а больше, чем у одной модели
11:45:07	 От  Семён Шафронов : Могли бы пожалуйста ещё раз объяснить, что такое выборка с возвращением?
11:45:45	 От  Константин Иванченко : снижает корреляцию моделей
11:45:48	 От  Tatiana : они будут скоррелированы
11:47:57	 От  Алексей : random forest?
11:50:24	 От  Tatiana : т.е. мы не из изначальной выборки берем подвыборки?
11:51:24	 От  Dmitry P. : простите, но можете ещё раз объяснить ещё раз про бутстррап...
11:51:50	 От  Семён Шафронов : Спасибо про выборку с возвращением понятно.
11:52:37	 От  Михаил Новиков : Хорошо было бы чуть чуть изменять каждый раз сэмпл, чтобы уменьшить переобучение.
11:53:01	 От  Максим Кобзев : Зачем вытаскивать семплы, а не взять и обучать на всем датасете, немного меня этот дотаяет для каждой новой модели (смещать или тп)
11:53:03	 От  Максим Кобзев : ?
11:53:33	 От  Максим Кобзев : А всё! Понял! Спасибо!
11:54:56	 От  Tatiana : бэггинг применяется когда все модели одного типа? например все модели деревья?
11:55:00	 От  Константин Иванченко : Существует ли какое-либо правило для выборки данных для сэмпла при бутстрапе?
11:56:17	 От  Александр Бобр : Правильно понимаю? - допустим у нас 10 значений в обуающей выборке. Мы обучаем на подмножествах 1-5, 2-6, 3-7, и т.д.?
12:06:50	 От  Nikita Syrov : Как обычно соотносится размер подпространства признаков с размером пространства признаков?
12:07:31	 От  Семён Шафронов : Сейчас важно понять, что такое CART-дерево? Если да, то что это?
12:07:57	 От  Nikita Syrov : Classification&RegressionTree
12:08:09	 От  Семён Шафронов : Никита, спасибо.
12:08:47	 От  Nikita Syrov : Это один из алгоритмов построения. Пример другого алгоритма - допустим, CHAID
12:09:14	 От  Семён Шафронов : Спасибо.
12:24:01	 От  Tatiana : интерпретируемость стремится к нулю
12:25:46	 От  Dmitry Novikov : То есть bagging - это параллельное подключение(обучение) моделей, 
boosting - последовательное подключение(обучение), 
а метамодель - параллельно-последовательное подключение уже наученных моделей с хитрой логикой выбора из показаний моделей внутри?
12:26:45	 От  Семён Шафронов : Является ли ансамблированием, если мы на одной модели (напр., XGB) разбили датасет на 2 части: все клиенты из Москвы, и все клиенты с Iphone. И потом оценили клиентов из Москвы с iPhone сначала одной, потом другой моделью.
12:46:10	 От  Алексей : А алгоритмы разные, или могут быть одни и те же с разными метапараметрами?
13:03:51	 От  Семён Шафронов : да
13:09:08	 От  Victoria : Я пропустила или мы решили не делать перерыв?
13:09:29	 От  Константин Иванченко : перерыв был, но был заполнен вопросами
13:09:33	 От  Максим Кобзев : Был перерыв 12 40 - 12 50
13:09:36	 От  Dmitry Novikov : Весь перерыв ушёл на объяснение blending
13:09:43	 От  Victoria : Аа, поняла))
13:09:54	 От  Victoria : нене, вопросов нет)
13:14:41	 От  Victoria : А можно еще раз пожалуйста, как в функцию генерации предсказаний мы отдаем знание, что нужны скорректированные предсказания или нет
13:15:28	 От  Victoria : скоррелированные*
13:17:12	 От  Dmitry Novikov : Если вы посчитаете к-т корреляции между "не скореллированными", я думаю, там 0.3-0.4 будет.
13:50:22	 От  Maria : Можно как-то управлять способом голосования?
13:51:03	 От  Семён Шафронов : Мб, веса расставлять каждому эксперту (soft_voting).
13:51:32	 От  Maria : спасибо!
13:52:47	 От  Ruslan Tskhovrebadze : А как сравнить полученные модели при мультиклассовой классификации если классов 4 или больше? Когда нет возможности построить график и оценить визуально
14:06:29	 От  Елена : При stacking обучать метамодель нужно на всей выборке (так как если объединить все кусочки валидаций с построения моделей, то получится вся выборка) или также, как и модели, с использованием кроссвалидации?
14:09:22	 От  Алексей : С ближайшими соседями тоже есть бустеры?
14:10:29	 От  Семён Шафронов : если я захочу сделать ансамбль из нейросети + xgb, лучше отдельно их запустить или можно через mlxtend?
14:13:09	 От  Семён Шафронов : спасибо.
14:17:05	 От  Семён Шафронов : А в метамодель что чаще подается от исходных моделей - predict (0 и 1) или predict_proba (вероятность)?
14:18:02	 От  Maria : Есть ли успешные примеры реализации в проде (не в соревнованиях) ансамблей моделей? Желательно в России :)
14:22:30	 От  Семён Шафронов : Спасибо)
14:23:26	 От  Maria : Спасибо!
14:23:48	 От  Dmitry Novikov : Спасибо большое :)
14:23:48	 От  Михаил Новиков : Спасибо!
14:23:49	 От  Tatiana : спасибо!
14:23:53	 От  Константин Иванченко : Спасибо!
