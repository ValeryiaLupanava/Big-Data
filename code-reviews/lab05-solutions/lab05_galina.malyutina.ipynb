{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import pymorphy2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfiles = [2561, 2049, 4, 2565, 2562, 2062, 3600, 3603, 3588, 534, 3607, 24, 25, 3612, 3102, 2591, 3804, 3627, 1580, 46, 1074, 2100, 3125, 56, 1593, 2485, 2621, 63, 3139, 2316, 610, 2121, 1098, 1099, 588, 2126, 2639, 88, 2140, 607, 1634, 2067, 612, 3175, 3178, 531, 1647, 1136, 1137, 1139, 1146, 1151, 22, 1710, 2692, 1160, 1674, 1560, 3729, 623, 3224, 153, 2206, 1183, 160, 3235, 3748, 684, 1651, 1711, 30, 1714, 2079, 695, 2232, 698, 189, 1227, 1025, 711, 3784, 2253, 2763, 3788, 1741, 2255, 1232, 3283, 383, 2775, 2265, 1829, 1244, 3300, 1767, 1257, 1320, 1747, 3830, 1152, 1279, 3030, 913, 3332, 1798, 2311, 386, 1289, 3340, 269, 1806, 274, 1425, 789, 2863, 1305, 2332, 800, 1825, 2339, 292, 293, 2855, 3368, 562, 2861, 3886, 2351, 2864, 307, 2356, 822, 1019, 3722, 2101, 3387, 2688, 829, 3904, 833, 2279, 3398, 2887, 336, 337, 3473, 854, 3420, 2398, 3426, 3939, 3940, 357, 2918, 1387, 2924, 879, 3442, 1396, 2421, 2427, 1407, 1345, 2438, 1928, 1419, 2456, 1054, 2030, 1937, 402, 2628, 2968, 409, 1437, 1956, 421, 1963, 943, 2481, 3509, 955, 2635, 2496, 455, 972, 3025, 1188, 3542, 3544, 3550, 482, 3699, 2812, 2534, 3566, 1521, 3570, 1525, 2043]\n",
    "\n",
    "text_test = []\n",
    "text_base = []\n",
    "for i in idfiles:\n",
    "    f = open('/data/share/lab05data/test_'+str(i)+\".txt\", 'r', encoding='utf-8')\n",
    "    t = f.readlines()\n",
    "    text_test.append(t)\n",
    "for j in range(1, 21):\n",
    "    s = open('/data/share/lab05data/base_'+str(j)+\".txt\", 'r', encoding='utf-8')\n",
    "    t2=s.readlines()\n",
    "    text_base.append(t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian','english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формирование весов \n",
    "cv = CountVectorizer(tokenizer=token_r)\n",
    "mx_tf = cv.fit_transform(base_text['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# частые слова\n",
    "words = [y[0] for y in sorted(cv.vocabulary_.items(), key=lambda x: x[1])]\n",
    "c_words = zip(words, sum(mx_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_words = sorted(c_words, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_w = set([x[0] for x in c_words[:6]])\n",
    "stop_w = list(stop_w | stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPING_SPACE_REGEX = re.compile(r'([^\\w]|[+])', re.UNICODE)\n",
    "m = pymorphy2.MorphAnalyzer()\n",
    "def simple_word_tokenize(text, _split=GROUPING_SPACE_REGEX.split):\n",
    "    return [t for t in _split(text.lower()) if t and not t.isspace()]\n",
    "\n",
    "def token_r(text):\n",
    "    words = simple_word_tokenize(text)\n",
    "    return [m.parse(x)[0].normal_form for x in words if len(x) >= 4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_base = [str (item) for item in text_base]\n",
    "text_test = [str (item) for item in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=token_r, stop_words=stop_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_tf = cv.fit_transform(text_base).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_entry = cv.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_m = cosine_similarity( new_entry,mx_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "mn = np.asarray([i.sum() for i in cos_m]).mean()\n",
    "\n",
    "defined = []\n",
    "other = []\n",
    "for i in cos_m:\n",
    "    if i.sum()>mn:\n",
    "        defined.append(idfiles[n])\n",
    "    else:\n",
    "        other.append(idfiles[n])    \n",
    "    n +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"defined\": [2561, 2049, 4, 2565, 3600, 3603, 3588, 534, 3607, 24, 3612, 3804, 3627, 2621, 3139, 2316, 1098, 1099, 2639, 88, 2140, 607, 531, 1647, 1136, 1137, 1139, 22, 2692, 1160, 1674, 3224, 153, 3748, 684, 30, 3784, 2763, 1741, 3283, 1244, 1767, 1747, 3830, 1152, 1279, 3030, 3332, 2311, 3340, 789, 2863, 2332, 800, 1825, 292, 2855, 3368, 2861, 3886, 2351, 2864, 822, 3387, 829, 3904, 833, 336, 3473, 854, 3426, 3940, 357, 2918, 1387, 2924, 879, 3442, 2421, 2427, 1407, 1928, 2030, 1937, 402, 2628, 421, 1963, 955, 455, 972, 1188, 3544, 2534, 3566, 2043], \"other\": [2562, 2062, 25, 3102, 2591, 1580, 46, 1074, 2100, 3125, 56, 1593, 2485, 63, 610, 2121, 588, 2126, 1634, 2067, 612, 3175, 3178, 1146, 1151, 1710, 1560, 3729, 623, 2206, 1183, 160, 3235, 1651, 1711, 1714, 2079, 695, 2232, 698, 189, 1227, 1025, 711, 2253, 3788, 2255, 1232, 383, 2775, 2265, 1829, 3300, 1257, 1320, 913, 1798, 386, 1289, 269, 1806, 274, 1425, 1305, 2339, 293, 562, 307, 2356, 1019, 3722, 2101, 2688, 2279, 3398, 2887, 337, 3420, 2398, 3939, 1396, 1345, 2438, 1419, 2456, 1054, 2968, 409, 1437, 1956, 943, 2481, 3509, 2635, 2496, 3025, 3542, 3550, 482, 3699, 2812, 1521, 3570, 1525]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# создаем словарь x:\n",
    "x = {\n",
    "\"defined\": defined,\n",
    "\"other\": other\n",
    "}\n",
    "# конвертируем в JSON: \n",
    "y = json.dumps(x)\n",
    "# в результате получаем строк JSON:\n",
    "y.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab05.json\", \"w\") as write_file:\n",
    "    json.dump(y, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
