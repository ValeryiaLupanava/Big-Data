{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T10:48:22.885685Z",
     "start_time": "2020-11-06T10:48:22.880311Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from snowballstemmer import RussianStemmer, EnglishStemmer\n",
    "import pymorphy2\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:33:32.777044Z",
     "start_time": "2020-11-09T08:33:32.743930Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"/data/share/lab05data/\"\n",
    "# Индексы, указанные в личном кабинете\n",
    "ids = set([2561, 514, 2570, 3085, 1040, 1046, 24, 3610, 3612, 859, 33, 945, 2087, 1885, 1580, 1074,\n",
    "           3639, 1593, 2485, 573, 63, 3649, 3139, 582, 2049, 1608, 1098, 1099, 2639, 3668, 1626, 607, \n",
    "           2656, 1121, 1635, 3178, 1644, 2158, 2159, 2238, 3699, 3700, 2682, 2173, 2174, 3203, 2692,\n",
    "           2184, 2187, 536, 3213, 3214, 3728, 3729, 658, 2195, 3222, 2199, 2200, 624, 672, 161, 1698,\n",
    "           3748, 711, 3239, 3244, 1710, 176, 3763, 1396, 1717, 695, 698, 1723, 1725, 702, 1215, 1217,\n",
    "           1320, 3267, 196, 710, 3783, 3272, 715, 2255, 724, 1758, 3808, 3300, 3818, 748, 3822, 751,\n",
    "           2288, 3180, 3317, 2806, 3831, 384, 623, 3030, 3842, 3850, 2316, 783, 2324, 1305, 2332, 1311, \n",
    "           1825, 292, 1831, 562, 812, 2863, 3376, 307, 820, 3551, 2361, 833, 2374, 2377, 1355, 3920,\n",
    "           337, 2386, 1367, 1308, 347, 861, 352, 3473, 3940, 870, 2921, 2924, 1524, 13, 2758, 372,\n",
    "           1909, 887, 888, 382, 1174, 897, 3137, 903, 1419, 579, 1006, 1937, 917, 406, 2456, 414, 2463,\n",
    "           3489, 668, 2981, 429, 2760, 1457, 3508, 3509, 1439, 2634, 3001, 3516, 445, 2379, 2496, 1697,\n",
    "           971, 972, 2510, 464, 2003, 1956, 3542, 2521, 2010, 1500, 2526, 479, 3041, 482, 2532, 485, \n",
    "           2023, 3564, 3566, 766, 2548, 3576, 3066, 1020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Круто, что разделил на английские и русские слова, это было моим запасным вариантом, если бы чекер не засчитал задание :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T09:59:21.256483Z",
     "start_time": "2020-11-06T09:59:21.195849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Стеммеры для русских и английских слов\n",
    "rs = RussianStemmer()\n",
    "es = EnglishStemmer()\n",
    "# Лемматизаторы для русских и английских слов\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Русские и английские стоп-слова\n",
    "stops_eng = set(stopwords.words(\"english\"))\n",
    "stops_rus = set(stopwords.words(\"russian\"))\n",
    "stops = stops_eng.union(stops_rus)\n",
    "\n",
    "# Функция определения английского слова\n",
    "def is_english(word):\n",
    "    if re.match(\"[a-z]\", word) is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Функция определения русского слова\n",
    "def is_russian(word):\n",
    "    if re.match(\"[а-я]\", word) is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Функция получения нормальной формы слова\n",
    "def get_normal_form(word):\n",
    "    # для английских слов\n",
    "    if is_english(word):\n",
    "        return lemmatizer.lemmatize(word)\n",
    "    # для русских слов\n",
    "    if is_russian(word):\n",
    "        return morph.parse(word)[0].normal_form\n",
    "    return \"\"\n",
    "\n",
    "# Функция получения стеммированной формы слова\n",
    "def stem_word(word):\n",
    "    # для английских слов\n",
    "    if is_english(word):\n",
    "        return es.stemWord(word)\n",
    "    # для русских слов\n",
    "    if is_russian(word):\n",
    "        return rs.stemWord(word)\n",
    "    return \"\"\n",
    "\n",
    "# Функция получения из текста файла набора (преобразованных) слов\n",
    "def text_to_wordlist( text, remove_stopwords=False, stemming=False, lemming=False ):\n",
    "    '''\n",
    "    Функция получения из текста файла набора (преобразованных) слов\n",
    "    \n",
    "    text: текст, str\n",
    "    remove_stopwords: флаг удаления стоп-слов, bool\n",
    "    stemming: флаг стемминга слов, bool\n",
    "    lemming: флаг лемматизации слов, bool\n",
    "    '''\n",
    "    # Достаем текст из файла, игнорируем тэги\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    # Оставляем только латинские и русские буквы\n",
    "    text = re.sub(\"[^a-zA-Zа-яА-Я]\",\" \", text)\n",
    "    words = text.lower().split()\n",
    "    # удаление стоп-слов\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if not w in stops]\n",
    "    # стемминг\n",
    "    if stemming:\n",
    "        words = [stem_word(word) for word in words]\n",
    "    # лемматизация\n",
    "    if lemming:\n",
    "        words = [get_normal_form(word) for word in words]\n",
    "\n",
    "    return(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По-хорошему надо файл закрывать после открытия. В целом, на таком количестве это некритично, но правила хорошего тона и тд. Еще, как мне кажется, проще было использовать датафреймы, чтобы потом не писать циклы, но это уже дело вкуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T09:59:21.169932Z",
     "start_time": "2020-11-06T09:57:15.867468Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/bd9/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda/envs/bd9/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# массив базовых текстов\n",
    "base_texts = []\n",
    "# словарь тестовых текстов (id -> text)\n",
    "test_texts = dict()\n",
    "# проход по файлам в указанной папке\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        text = open(file_path).read()\n",
    "        # получение набора слов из файла (удаление стоп-слов и лемматизация)\n",
    "        wordlist = text_to_wordlist(text, remove_stopwords=True, stemming=False, lemming=True)\n",
    "        if \"base\" in file:\n",
    "            base_texts.append( wordlist )\n",
    "        if \"test\" in file:\n",
    "            # получение id текста из имени файла\n",
    "            text_id = int(file.split('.')[0].split('_')[1])\n",
    "            if text_id in ids:\n",
    "                test_texts[text_id] = wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T09:59:28.778065Z",
     "start_time": "2020-11-06T09:59:28.774026Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "#                              max_features = 5000\n",
    "                            ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень понятно, зачем присваивание all_texts = base_texts. И по-моему так же, как у меня - тут используется fit_transform, а потом опять transform. Можно либо тут оставить только fit, либо сразу делать fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T10:24:19.797456Z",
     "start_time": "2020-11-06T10:24:19.771264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x1054 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2541 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем CountVectorizer на базовых текстах\n",
    "all_texts = base_texts\n",
    "vectorizer.fit_transform(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T10:25:54.575382Z",
     "start_time": "2020-11-06T10:25:54.536499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Получаем векторное представление базовых и тестовых текстов\n",
    "base_features = vectorizer.transform(base_texts)\n",
    "test_features = vectorizer.transform([test_texts[id_] for id_ in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T10:27:14.796972Z",
     "start_time": "2020-11-06T10:27:14.789202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Считаем косинусную близость между текстами и суммируем полученные близости для каждого из тестового текста\n",
    "similarities = cosine_similarity(base_features, test_features).sum(axis=0)\n",
    "# Находим среднюю сумму косинусных близостей\n",
    "mean_s = similarities.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T10:48:09.029000Z",
     "start_time": "2020-11-06T10:48:09.024416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Определяем принадлежность тестового текста к заданной тематике, сравнивая с пороговым значением\n",
    "result = {\"defined\": [], \"other\": []}\n",
    "for i, id_ in enumerate(ids):\n",
    "    if similarities[i] > mean_s:\n",
    "        result[\"defined\"].append(id_)\n",
    "    else:\n",
    "        result[\"other\"].append(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T10:49:40.831449Z",
     "start_time": "2020-11-06T10:49:40.826201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Вывод результата в файл\n",
    "with open(\"lab05.json\", \"w\") as f:\n",
    "    f.write( json.dumps(result) )"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
