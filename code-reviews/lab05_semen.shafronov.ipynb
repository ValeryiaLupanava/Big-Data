{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Галина, привет.\n",
    "##Круто, что используешь столько библиотек.\n",
    "##Возможно, если разобъешь их на группы и подчистишь те, которые не использовала (import gensim\n",
    "## import logging), будет удобнее читать.\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import pymorphy2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Включила при чтении кодировку ЮТФ-8. Зачет!\n",
    "\n",
    "idfiles = [2561, 2049, 4, 2565, 2562, 2062, 3600, 3603, 3588, 534, 3607, 24, 25, 3612, 3102, 2591, 3804, 3627, 1580, 46, 1074, 2100, 3125, 56, 1593, 2485, 2621, 63, 3139, 2316, 610, 2121, 1098, 1099, 588, 2126, 2639, 88, 2140, 607, 1634, 2067, 612, 3175, 3178, 531, 1647, 1136, 1137, 1139, 1146, 1151, 22, 1710, 2692, 1160, 1674, 1560, 3729, 623, 3224, 153, 2206, 1183, 160, 3235, 3748, 684, 1651, 1711, 30, 1714, 2079, 695, 2232, 698, 189, 1227, 1025, 711, 3784, 2253, 2763, 3788, 1741, 2255, 1232, 3283, 383, 2775, 2265, 1829, 1244, 3300, 1767, 1257, 1320, 1747, 3830, 1152, 1279, 3030, 913, 3332, 1798, 2311, 386, 1289, 3340, 269, 1806, 274, 1425, 789, 2863, 1305, 2332, 800, 1825, 2339, 292, 293, 2855, 3368, 562, 2861, 3886, 2351, 2864, 307, 2356, 822, 1019, 3722, 2101, 3387, 2688, 829, 3904, 833, 2279, 3398, 2887, 336, 337, 3473, 854, 3420, 2398, 3426, 3939, 3940, 357, 2918, 1387, 2924, 879, 3442, 1396, 2421, 2427, 1407, 1345, 2438, 1928, 1419, 2456, 1054, 2030, 1937, 402, 2628, 2968, 409, 1437, 1956, 421, 1963, 943, 2481, 3509, 955, 2635, 2496, 455, 972, 3025, 1188, 3542, 3544, 3550, 482, 3699, 2812, 2534, 3566, 1521, 3570, 1525, 2043]\n",
    "\n",
    "text_test = []\n",
    "text_base = []\n",
    "for i in idfiles:\n",
    "    f = open('/data/share/lab05data/test_'+str(i)+\".txt\", 'r', encoding='utf-8')\n",
    "    t = f.readlines()\n",
    "    text_test.append(t)\n",
    "for j in range(1, 21):\n",
    "    s = open('/data/share/lab05data/base_'+str(j)+\".txt\", 'r', encoding='utf-8')\n",
    "    t2=s.readlines()\n",
    "    text_base.append(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##круто, что сразу одной командой собрала все стоп-слова\n",
    "stops = set(stopwords.words('russian','english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c4e101f89ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# формирование весов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmx_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'base_text' is not defined"
     ]
    }
   ],
   "source": [
    "## отлично, что используешь самописную функцию (token_r) в качестве токенайзера. \n",
    "## возможно, эту функцию стоит объявить до того, как будешь её использовать в CountVectorizer\n",
    "## иначе возникает ошибка ниже\n",
    "## у тебя не возникала возможно потому, что была загружена в память командой ниже\n",
    "## для отладки можно сделать kernel/restart и прогнать весь ноутбук с нуля \n",
    "\n",
    "# формирование весов \n",
    "cv = CountVectorizer(tokenizer=token_r)\n",
    "mx_tf = cv.fit_transform(base_text['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0c4a1ce7abd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# частые слова\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmx_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "# частые слова\n",
    "words = [y[0] for y in sorted(cv.vocabulary_.items(), key=lambda x: x[1])]\n",
    "c_words = zip(words, sum(mx_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d963c67ddf7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'c_words' is not defined"
     ]
    }
   ],
   "source": [
    "c_words = sorted(c_words, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-31466eb9c037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# превосходно! Взяла, насколько я понял, топ 6 слов по встречаемости\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# и добавила в стоп-слова. И это сработало.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstop_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstop_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_w\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mstops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c_words' is not defined"
     ]
    }
   ],
   "source": [
    "## превосходно! Взяла, насколько я понял, топ 6 слов по встречаемости\n",
    "## и добавила в стоп-слова. \n",
    "stop_w = set([x[0] for x in c_words[:6]])\n",
    "stop_w = list(stop_w | stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPING_SPACE_REGEX = re.compile(r'([^\\w]|[+])', re.UNICODE)\n",
    "m = pymorphy2.MorphAnalyzer()\n",
    "def simple_word_tokenize(text, _split=GROUPING_SPACE_REGEX.split):\n",
    "    return [t for t in _split(text.lower()) if t and not t.isspace()]\n",
    "\n",
    "def token_r(text):\n",
    "    words = simple_word_tokenize(text)\n",
    "    return [m.parse(x)[0].normal_form for x in words if len(x) >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_base = [str (item) for item in text_base]\n",
    "text_test = [str (item) for item in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0198580f0925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_w' is not defined"
     ]
    }
   ],
   "source": [
    "## имеет ли смысл дважды инициализировать класс cv?\n",
    "cv = CountVectorizer(tokenizer=token_r, stop_words=stop_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_tf = cv.fit_transform(text_base).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## то, что надо! Используешь прием показанный Петром. \n",
    "new_entry = cv.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_m = cosine_similarity( new_entry,mx_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "## у тебя перебирает cos_m, но далее в цилке ничего из него не берет\n",
    "## отсюда вывод, что он нужен только для определения кол-ва элементов в cos_m\n",
    "## как альтернатива счетчику n, можно использовать range(len(объект)). И тогда n не надо использовать)\n",
    "\n",
    "n = 0\n",
    "mn = np.asarray([i.sum() for i in cos_m]).mean()\n",
    "\n",
    "defined = []\n",
    "other = []\n",
    "for i in cos_m:\n",
    "    if i.sum()>mn:\n",
    "        defined.append(idfiles[n])\n",
    "    else:\n",
    "        other.append(idfiles[n])    \n",
    "    n +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"defined\": [2561, 2049, 4, 2565, 3600, 3603, 3588, 534, 3607, 24, 3612, 3804, 3627, 2621, 3139, 2316, 1098, 1099, 2639, 88, 2140, 607, 531, 1647, 1136, 1137, 1139, 22, 2692, 1160, 1674, 3224, 153, 3748, 684, 30, 3784, 2763, 1741, 3283, 1244, 1767, 1747, 3830, 1152, 1279, 3030, 3332, 2311, 3340, 789, 2863, 2332, 800, 1825, 292, 2855, 3368, 2861, 3886, 2351, 2864, 822, 3387, 829, 3904, 833, 336, 3473, 854, 3426, 3940, 357, 2918, 1387, 2924, 879, 3442, 2421, 2427, 1407, 1928, 2030, 1937, 402, 2628, 421, 1963, 955, 455, 972, 1188, 3544, 2534, 3566, 2043], \"other\": [2562, 2062, 25, 3102, 2591, 1580, 46, 1074, 2100, 3125, 56, 1593, 2485, 63, 610, 2121, 588, 2126, 1634, 2067, 612, 3175, 3178, 1146, 1151, 1710, 1560, 3729, 623, 2206, 1183, 160, 3235, 1651, 1711, 1714, 2079, 695, 2232, 698, 189, 1227, 1025, 711, 2253, 3788, 2255, 1232, 383, 2775, 2265, 1829, 3300, 1257, 1320, 913, 1798, 386, 1289, 269, 1806, 274, 1425, 1305, 2339, 293, 562, 307, 2356, 1019, 3722, 2101, 2688, 2279, 3398, 2887, 337, 3420, 2398, 3939, 1396, 1345, 2438, 1419, 2456, 1054, 2968, 409, 1437, 1956, 943, 2481, 3509, 2635, 2496, 3025, 3542, 3550, 482, 3699, 2812, 1521, 3570, 1525]}\n"
     ]
    }
   ],
   "source": [
    "### спасибо, что сохранила в ноутбуке результаты вывода. Так нагляднее и проще понимать код.\n",
    "\n",
    "import json\n",
    "\n",
    "# создаем словарь x:\n",
    "x = {\n",
    "\"defined\": defined,\n",
    "\"other\": other\n",
    "}\n",
    "# конвертируем в JSON: \n",
    "y = json.dumps(x)\n",
    "# в результате получаем строк JSON:\n",
    "y.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab05.json\", \"w\") as write_file:\n",
    "    json.dump(y, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
