{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Natasha Pritykovskaya Streaming app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bd-master.newprolab.com:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Natasha Pritykovskaya Streaming app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f221fb40c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "KAFKA_BOOTSTRAP = \"bd-master.newprolab.com:6667\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset\n",
    "\n",
    "Подготовим тестовый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|                                                                                value|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|{ \"name\":\"Moscow\", \"country\":\"Russia\", \"continent\": \"Europe\", \"population\": 12380664}|\n",
      "|                                               { \"name\":\"Madrid\", \"country\":\"Spain\" }|\n",
      "| { \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}|\n",
      "|{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}|\n",
      "|                     { \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }|\n",
      "| { \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }|\n",
      "| { \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }|\n",
      "|                                                 { \"name\":\"New York, \"country\":\"USA\",|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = \\\n",
    "\"\"\"{ \"name\":\"Moscow\", \"country\":\"Russia\", \"continent\": \"Europe\", \"population\": 12380664}\n",
    "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
    "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
    "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
    "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"New York, \"country\":\"USA\",\"\"\"\n",
    "\n",
    "splited = list(map(lambda x: (x,), test_data.split(\"\\n\")))\n",
    "\n",
    "cities_df = spark.createDataFrame(splited, schema=\"\"\"value: string\"\"\")\n",
    "\n",
    "cities_df.printSchema()\n",
    "\n",
    "cities_df.show(10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with kafka using static DF API\n",
    "\n",
    "Работать с кафкой можно как с использованием обычных статических датафреймов, так и стримовых. Начнем со статических."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce static DF to Kafka\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#writing-the-output-of-batch-queries-to-kafka\n",
    "\n",
    "Запись данных в кафку с точки зрения API мало чем отличается от записи в файл. Достаточно указать нужный `format` и задать параметры подключения к кафке через `option` или `options`.\n",
    "\n",
    "Одним из требований является структура датафрейма - в нем должны быть колонки `value` (обязательно) и `key`, `topic` (опционально). В нашем датафрейме одна единственная колонка `value`, которая содержит JSON строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to Kafka\n"
     ]
    }
   ],
   "source": [
    "cities_df \\\n",
    "    .withColumn(\"topic\", lit(\"test_topic_100\")) \\\n",
    "    .write.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data has been written to Kafka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume static DF from Kafka\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#creating-a-kafka-source-for-batch-queries\n",
    "\n",
    "Чтение выполняется аналогично записи: необходимо указать `format` и параметры подключения к топику. Датафрейм, созданный на основе топика (топиков) кафки всегда имеет одну структуру и набор колонок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+-------------------------+--------------+---------+------+-----------------------+-------------+\n",
      "| key|                    value|         topic|partition|offset|              timestamp|timestampType|\n",
      "+----+-------------------------+--------------+---------+------+-----------------------+-------------+\n",
      "|null|[7B 20 22 6E 61 6D 65 ...|test_topic_100|        0|     0|2020-12-01 12:47:19.588|            0|\n",
      "|null|[7B 20 22 6E 61 6D 65 ...|test_topic_100|        0|     1|2020-12-01 12:47:19.685|            0|\n",
      "|null|[7B 20 22 6E 61 6D 65 ...|test_topic_100|        0|     2|2020-12-01 12:47:19.701|            0|\n",
      "|null|[7B 20 22 6E 61 6D 65 ...|test_topic_100|        0|     3|2020-12-01 12:47:19.601|            0|\n",
      "|null|[7B 20 22 6E 61 6D 65 ...|test_topic_100|        0|     4|2020-12-01 12:47:19.701|            0|\n",
      "+----+-------------------------+--------------+---------+------+-----------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_from_kafka = \\\n",
    "    spark \\\n",
    "        .read.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "        .option(\"subscribe\", \"test_topic_100\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "static_from_kafka.printSchema()\n",
    "\n",
    "static_from_kafka.show(5, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом необходимо помнить, что мы сейчас работаем с кафкой, используя статический датафрейм и оффсеты здесь никак не используются. При каждой запуске `static_from_kafka.show(5, 25)` мы будем читать одни и те же данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run queries on static DF from Kafka\n",
    "\n",
    "В части выполнения SQL запросов здесь нет никаких отличий от других источников:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|{ \"name\":\"Moscow\", \"country\":\"Russia\", \"contine...|\n",
      "|{ \"name\":\"Barselona\", \"country\":\"Spain\", \"conti...|\n",
      "|{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent...|\n",
      "|            { \"name\":\"Madrid\", \"country\":\"Spain\" }|\n",
      "|{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deserialize binary value to string\n",
    "deserialized = \\\n",
    "    static_from_kafka \\\n",
    "        .select(col(\"value\").cast(\"string\").alias(\"value\")) \\\n",
    "\n",
    "deserialized.show(5, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из частых задач при работе с кафкой - это десериализация `value` (и иногда `key`) сообщения. Кафка не знает ничего о типах данных в ваших сообщениях, которые хранятся в ней в бинарном виде. При чтении из кафки мы десериализуем данные в нужный формат, а при записи - сериализуем. В нашем случае мы используем JSON.\n",
    "\n",
    "Для работа с JSON строкой в спарке есть 3 функции: `json_tuple`, `from_json`, `get_json_object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Russia |Moscow   |12380664  |\n",
      "|Europe   |Spain  |Barselona|null      |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "|null     |Spain  |Madrid   |null      |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "+---------+-------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse JSON strings using different methods\n",
    "\n",
    "# json_tuple\n",
    "parsed = \\\n",
    "    deserialized \\\n",
    "        .select(json_tuple(col(\"value\"), \"continent\", \"country\", \"name\", \"population\")\n",
    "                .alias(\"continent\", \"country\", \"name\", \"population\"))\n",
    "\n",
    "# from_json\n",
    "parsed = \\\n",
    "    deserialized \\\n",
    "        .select(col(\"value\").cast(\"string\").alias(\"value\")) \\\n",
    "        .select(from_json(col(\"value\"), \"\"\"continent string, country string, name string, population long\"\"\")\n",
    "                .alias(\"value\")) \\\n",
    "        .select(col(\"value.*\"))\n",
    "\n",
    "parsed = \\\n",
    "    deserialized \\\n",
    "        .select(\n",
    "            get_json_object(col(\"value\"), \"$.continent\").alias(\"continent\"),\n",
    "            get_json_object(col(\"value\"), \"$.country\").alias(\"country\"),\n",
    "            get_json_object(col(\"value\"), \"$.name\").alias(\"name\"),\n",
    "            get_json_object(col(\"value\"), \"$.population\").alias(\"population\"),\n",
    "        )\n",
    "\n",
    "parsed.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanse data\n",
    "\n",
    "После парсинга JSON мы можем очистить наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "|n/a      |Spain  |Madrid   |0         |\n",
      "|Europe   |Spain  |Barselona|0         |\n",
      "|Europe   |Russia |Moscow   |12380664  |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean = \\\n",
    "    parsed \\\n",
    "        .na.drop(\"all\") \\\n",
    "        .dropDuplicates() \\\n",
    "        .na.fill({\"continent\": \"n/a\", \"population\": 0})\n",
    "\n",
    "clean.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем базовый агрегат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------------+\n",
      "|country|count|total_population|\n",
      "+-------+-----+----------------+\n",
      "|Russia |1    |12380664        |\n",
      "|Germany|1    |3490105         |\n",
      "|France |1    |2196936         |\n",
      "|Spain  |2    |0               |\n",
      "|Egypt  |1    |11922948        |\n",
      "+-------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run aggregation query\n",
    "agg = \\\n",
    "    clean \\\n",
    "        .groupBy(\"country\") \\\n",
    "        .agg(count(\"*\").alias(\"count\"), sum(\"population\").cast(\"long\").alias(\"total_population\"))\n",
    "\n",
    "agg.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with kafka using Streaming DF API\n",
    "\n",
    "До этого мы работали с кафкой, используя статические датафреймы. Данный подход удобен, когда вам нужно изучить данные, которые хранятся в кафке или выполнить одноразовую задачу, к которой вы не вернетесь в будущем. Однако для построения стриминг пайплайна используются стриминг датафреймы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "True\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_from_kafka = \\\n",
    "    spark \\\n",
    "        .readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "        .option(\"subscribe\", \"test_topic_100\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "print(type(streaming_from_kafka))\n",
    "print(streaming_from_kafka.isStreaming)\n",
    "\n",
    "streaming_from_kafka.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание стриминг датафрейма и применение SQL к нему является ленивой операцией. Фактического чтения данных из кафки не происходит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = \\\n",
    "    streaming_from_kafka \\\n",
    "        .select(col(\"value\").cast(\"string\").alias(\"value\")) \\\n",
    "        .select(json_tuple(col(\"value\"), \"continent\", \"country\", \"name\", \"population\")\n",
    "                    .alias(\"continent\", \"country\", \"name\", \"population\")) \\\n",
    "        .na.drop(\"all\") \\\n",
    "            .dropDuplicates() \\\n",
    "            .na.fill({\"continent\": \"n/a\", \"population\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы запустить наш стрим, необходимо создать `StreamingQuery` с помощью `writeStream` и `start()`.\n",
    "В нашем случае мы используем `triger(once=True)`. Это означает, что стрим прочитает один батч (некоторое количество данных из кафки) и завершит свою работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f2212eb2a58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = clean \\\n",
    "        .writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(once=True) \\\n",
    "        .start()\n",
    "\n",
    "# .trigger(once=True) means that spark will trigger only one batch and then stop the stream\n",
    "# this is mode is useful when writing tests\n",
    "\n",
    "sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот вариант удобен для написать тестов и исследования данных. Для того, чтобы создать стрим, который не будет завершаться после обработки первого батча, необходимо использовать другой `trigger`. Создадим новый стрим на базе генератора значений `rate`. Наш стрим будет генерировать данные на основе счетчика и записывать их в топик кафки. Батчи будут запускаться каждые 20 сек. Поскольку мы создаем стрим `rate` > `kafka`, то никакого видимого результата в консоли мы не увидим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- foo: string (nullable = false)\n",
      " |-- bar: string (nullable = false)\n",
      "\n",
      "['id', 'foo', 'bar']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "test_df = spark.range(0,10).withColumn(\"foo\", lit(\"foo\")).withColumn(\"bar\", lit(\"bar\"))\n",
    "test_df.printSchema()\n",
    "\n",
    "\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from_rate = spark.readStream.format(\"rate\").load()\n",
    "\n",
    "from_rate.printSchema()\n",
    "\n",
    "to_kafka = \\\n",
    "    from_rate \\\n",
    "        .select(to_json(struct(*from_rate.columns)).alias(\"value\")) \\\n",
    "        .withColumn(\"topic\", lit(\"test_topic_200\")) \\\n",
    "        .writeStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "        .option(\"checkpointLocation\", \"chk_1\") \\\n",
    "        .trigger(processingTime=\"20 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "# .trigger(processingTime=\"20 seconds\") means that spark will trigger batch every \n",
    "# 20 seconds and process new data. This mode ensure your stream will never end unless error\n",
    "# will occur\n",
    "# \n",
    "# .option(\"checkpointLocation\", \"chk_1\") specifies location to persist stream state which \n",
    "# allows stream to be restarted from the point it stopped after intentional shut down or failure\n",
    "# If checkpoint does not exist, stream will start with initial options like \"earliest\" or \"latest\" offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isDataAvailable': False,\n",
      " 'isTriggerActive': True,\n",
      " 'message': 'Getting offsets from KafkaV2[Subscribe[test_topic_100]]'}\n",
      "{'isDataAvailable': False,\n",
      " 'isTriggerActive': False,\n",
      " 'message': 'Initializing sources'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for i in spark.streams.active:\n",
    "    pprint(i.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in spark.streams.active:\n",
    "#     if \"KafkaV2\" in i.lastProgress[\"sources\"][0][\"description\"]:\n",
    "#         i.stop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.parquet(\"output/test_parquet/*\")\n",
    "# df.printSchema()\n",
    "# df.show()\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим еще один стрим `kafka` > `console`. Данные стримы работают независимо друг от друга в рамках одного спарк приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_kafka = \\\n",
    "    spark \\\n",
    "        .readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "        .option(\"subscribe\", \"test_topic_200\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "to_console = \\\n",
    "    from_kafka \\\n",
    "        .select(col(\"value\").cast(\"string\").alias(\"value\")) \\\n",
    "        .select(json_tuple(col(\"value\"), \"value\", \"timestamp\")\n",
    "                .alias(\"value\", \"timestamp\")) \\\n",
    "        .writeStream.format(\"console\") \\\n",
    "        .option(\"checkpointLocation\", \"chk_2\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_parquet = \\\n",
    "    from_kafka \\\n",
    "    .select(col(\"value\").cast(\"string\").alias(\"value\")) \\\n",
    "    .select(json_tuple(col(\"value\"), \"value\", \"timestamp\")\n",
    "        .alias(\"value\", \"timestamp\")) \\\n",
    "    .writeStream.format(\"parquet\") \\\n",
    "    .option(\"path\", \"data_1\") \\\n",
    "    .option(\"checkpointLocation\", \"chk_3\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 items\n",
      "drwxr-xr-x   - natalya.pritykovskaya natalya.pritykovskaya          0 2020-12-01 18:22 data_1/_spark_metadata\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        887 2020-12-01 18:19 data_1/part-00000-01acba55-86e8-417c-9ccf-e8c28c674666-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        889 2020-12-01 18:20 data_1/part-00000-043c6e69-5ac1-40e5-9f57-3dc5353265d9-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        827 2020-12-01 18:22 data_1/part-00000-07c7b928-2508-4215-bd96-caef42220be6-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        818 2020-12-01 18:17 data_1/part-00000-0ff02336-feae-454b-a563-bdf8c7974ea2-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        950 2020-12-01 18:20 data_1/part-00000-1a97d611-25c1-4cd9-8545-4062c5109f76-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya     170280 2020-12-01 18:14 data_1/part-00000-21daa229-e088-4657-a7a0-2ced91919e67-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        891 2020-12-01 18:22 data_1/part-00000-2c02ded4-6381-448e-8980-4fa827fdd55f-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        900 2020-12-01 18:20 data_1/part-00000-393965a1-fdb2-4857-8d1e-30a49686ba0c-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        797 2020-12-01 18:19 data_1/part-00000-4508ac03-4b61-412e-a4f1-f2ddf7ff3909-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        894 2020-12-01 18:16 data_1/part-00000-5489c2b7-3f7c-4ec9-a07d-8f07bbf8b17f-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        958 2020-12-01 18:15 data_1/part-00000-566c30e3-50fb-4515-b1cd-f969a0342b33-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        889 2020-12-01 18:22 data_1/part-00000-60bd450e-0752-4e59-a406-f34b0b9de3d1-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        888 2020-12-01 18:17 data_1/part-00000-6e547bdf-8d7a-4716-8ff9-46527d35ef66-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        957 2020-12-01 18:22 data_1/part-00000-7481942f-e525-45cf-b2c2-5d2b0d3d7bc4-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        911 2020-12-01 18:18 data_1/part-00000-76ee4687-95e9-4c4e-9eae-fb448651d97d-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        818 2020-12-01 18:20 data_1/part-00000-8473c31a-4ae7-4319-92d1-63d1a03066ca-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        887 2020-12-01 18:16 data_1/part-00000-900d6e91-593e-4923-ba3d-28ada6c9b075-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        827 2020-12-01 18:19 data_1/part-00000-91039957-76d8-45a8-bed2-e4ca8d71d167-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        920 2020-12-01 18:19 data_1/part-00000-987b4fe2-d0d5-4a05-b2f7-5699e81a6748-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        827 2020-12-01 18:16 data_1/part-00000-a782b564-2f76-4254-b02d-83966127f3ae-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        774 2020-12-01 18:21 data_1/part-00000-aa4d226f-978c-43b0-8dd6-bf4c44c93bf3-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        827 2020-12-01 18:16 data_1/part-00000-ab610da2-c61b-4396-b215-ce46fb583a54-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        951 2020-12-01 18:15 data_1/part-00000-ac753396-523f-4750-a7d2-a9276b4bfd77-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        827 2020-12-01 18:18 data_1/part-00000-b60f374d-517d-45da-8a50-bd19088f7587-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        827 2020-12-01 18:14 data_1/part-00000-b8d64203-e5a8-45b7-8e1f-b4cf921f7684-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        892 2020-12-01 18:15 data_1/part-00000-b9586440-a808-4cb9-af9e-8236e719a056-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        948 2020-12-01 18:21 data_1/part-00000-ba805403-2a38-4efb-8d4e-f79eaff5510a-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        807 2020-12-01 18:18 data_1/part-00000-c1d13377-d62f-4e4d-b167-bda2809b1570-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        829 2020-12-01 18:17 data_1/part-00000-c4d69a29-e8ea-49c7-a6a2-d376c23d9959-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        893 2020-12-01 18:14 data_1/part-00000-c694161b-c6a9-419b-840f-63c806419a50-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        904 2020-12-01 18:21 data_1/part-00000-d0173ab1-2c60-4b6d-a19a-9ee9498e5151-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        956 2020-12-01 18:16 data_1/part-00000-d266735b-12e1-4761-97e1-4a099da376b8-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        830 2020-12-01 18:20 data_1/part-00000-d7100d59-6809-43ca-99fc-e1056e10ecea-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        934 2020-12-01 18:21 data_1/part-00000-dab3f702-0ea7-4a24-beb4-124e7058b1e4-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        954 2020-12-01 18:18 data_1/part-00000-db87e6ad-27b7-4eeb-9665-cb83de95bee2-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        895 2020-12-01 18:17 data_1/part-00000-e0ffa6f2-c326-46c7-8aed-db422fa7fc24-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        827 2020-12-01 18:15 data_1/part-00000-e9b34ca4-ea7d-4163-a28c-3e99178ccbbf-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        819 2020-12-01 18:17 data_1/part-00000-eaddc784-afa5-4ceb-95bd-a002cbd96c00-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        894 2020-12-01 18:18 data_1/part-00000-ec0bfbfc-c50e-4f40-a077-bb540bccaea7-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        827 2020-12-01 18:22 data_1/part-00000-f661a159-d827-4f2d-8077-bc4384fc1bae-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        953 2020-12-01 18:19 data_1/part-00000-f7bc036d-70b8-4c0a-9e6d-14be67034635-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        890 2020-12-01 18:17 data_1/part-00000-fabaa6bf-006f-4e7d-b6be-96a45c459afd-c000.snappy.parquet\n",
      "-rw-r--r--   2 natalya.pritykovskaya natalya.pritykovskaya        818 2020-12-01 18:21 data_1/part-00000-fdecc18b-44fe-4cfe-b8ee-ac8c34d8fa04-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом мы можем создать произвольное количество стримов. Одним из недостатков данного API является то, что если мы сделали один стриминг датафрейм из кафки и создали несколько стримов с записью в разные синки (от англ. sink), мы фактически создадим несколько назависимых стримов и будем читать кафку несколько раз. В большинстве случаев это непозволительная роскошь, т.к. такой подход созает лишнюю нагрузку на кафку и сеть. Для решения этой проблемы можно использовать синк `foreachBatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_func(batch_df, batch_id):\n",
    "    batch_df.write.mode(\"append\").parquet(\"test_parquet/{id}\".format(id=batch_id))\n",
    "    batch_df.write.mode(\"append\").orc(\"test_orc/{id}\".format(id=batch_id))\n",
    "\n",
    "\n",
    "to_parquet_2 = \\\n",
    "    from_kafka \\\n",
    "        .select(col(\"value\").cast(\"string\").alias(\"value\")) \\\n",
    "        .select(json_tuple(col(\"value\"), \"value\", \"timestamp\")\n",
    "                .alias(\"value\", \"timestamp\")) \\\n",
    "        .writeStream.foreachBatch(batch_func) \\\n",
    "        .option(\"checkpointLocation\", \"chk_4\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся в корректности записанных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+-----+-----------------------------+\n",
      "|value|timestamp                    |\n",
      "+-----+-----------------------------+\n",
      "|0    |2020-12-01T12:47:56.252+03:00|\n",
      "|1    |2020-12-01T12:47:57.252+03:00|\n",
      "|2    |2020-12-01T12:47:58.252+03:00|\n",
      "|3    |2020-12-01T12:47:59.252+03:00|\n",
      "|6    |2020-12-01T12:48:02.252+03:00|\n",
      "|5    |2020-12-01T12:48:01.252+03:00|\n",
      "|4    |2020-12-01T12:48:00.252+03:00|\n",
      "|7    |2020-12-01T12:48:03.252+03:00|\n",
      "|10   |2020-12-01T12:48:06.252+03:00|\n",
      "|8    |2020-12-01T12:48:04.252+03:00|\n",
      "+-----+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orc_file = spark.read.orc(\"test_orc/*\")\n",
    "orc_file.printSchema()\n",
    "orc_file.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+-----+-----------------------------+\n",
      "|value|timestamp                    |\n",
      "+-----+-----------------------------+\n",
      "|0    |2020-12-01T12:47:56.252+03:00|\n",
      "|1    |2020-12-01T12:47:57.252+03:00|\n",
      "|2    |2020-12-01T12:47:58.252+03:00|\n",
      "|3    |2020-12-01T12:47:59.252+03:00|\n",
      "|6    |2020-12-01T12:48:02.252+03:00|\n",
      "|5    |2020-12-01T12:48:01.252+03:00|\n",
      "|4    |2020-12-01T12:48:00.252+03:00|\n",
      "|7    |2020-12-01T12:48:03.252+03:00|\n",
      "|10   |2020-12-01T12:48:06.252+03:00|\n",
      "|8    |2020-12-01T12:48:04.252+03:00|\n",
      "+-----+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_file = spark.read.parquet(\"test_parquet/*\")\n",
    "parquet_file.printSchema()\n",
    "parquet_file.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All streams has been stopped!\n"
     ]
    }
   ],
   "source": [
    "# Stop all running streaming queries\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "print(\"All streams has been stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "from_rate_2 = spark.readStream.format(\"rate\").load()\n",
    "with_pmod = from_rate_2.withColumn(\"pm\", expr(\"\"\"pmod(value, 3)\"\"\")).groupBy(col(\"pm\")).count()\n",
    "plus_one = with_pmod.withColumn(\"pm\", col(\"pm\")).filter(col(\"pm\") != 1)\n",
    "\n",
    "sq_2 = plus_one \\\n",
    "    .writeStream.format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"chk_13\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\": \"Processing new data\", \"isDataAvailable\": true, \"isTriggerActive\": true}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Get status of all streaming queries\n",
    "for s in spark.streams.active:\n",
    "    print(json.dumps(s.status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "{'batchId': 10,\n",
      " 'durationMs': {'addBatch': 5119,\n",
      "                'getBatch': 1,\n",
      "                'getEndOffset': 0,\n",
      "                'queryPlanning': 35,\n",
      "                'setOffsetRange': 0,\n",
      "                'triggerExecution': 5226,\n",
      "                'walCommit': 30},\n",
      " 'id': 'ce38a9f2-92d8-4bd1-909e-022b8709f3ba',\n",
      " 'inputRowsPerSecond': 1.1545122185876466,\n",
      " 'name': None,\n",
      " 'numInputRows': 6,\n",
      " 'processedRowsPerSecond': 1.148105625717566,\n",
      " 'runId': 'e7b992eb-1b4e-495d-889f-f904ffb5fc38',\n",
      " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@2b32e5a9'},\n",
      " 'sources': [{'description': 'RateStreamV2[rowsPerSecond=1, '\n",
      "                             'rampUpTimeSeconds=0, numPartitions=default',\n",
      "              'endOffset': 88,\n",
      "              'inputRowsPerSecond': 1.1545122185876466,\n",
      "              'numInputRows': 6,\n",
      "              'processedRowsPerSecond': 1.148105625717566,\n",
      "              'startOffset': 82}],\n",
      " 'stateOperators': [{'customMetrics': {'loadedMapCacheHitCount': 4000,\n",
      "                                       'loadedMapCacheMissCount': 0,\n",
      "                                       'stateOnCurrentVersionSizeBytes': 17751},\n",
      "                     'memoryUsedBytes': 80359,\n",
      "                     'numRowsTotal': 2,\n",
      "                     'numRowsUpdated': 2}],\n",
      " 'timestamp': '2020-12-01T15:30:23.677Z'}\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Get last progress of each streaming query\n",
    "for s in spark.streams.active:\n",
    "    print(\"#\" * 50)\n",
    "    pprint.pprint(s.lastProgress)\n",
    "    print(\"#\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
