{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions</a></span></li><li><span><a href=\"#Read-data\" data-toc-modified-id=\"Read-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Read data</a></span></li><li><span><a href=\"#Preprocess\" data-toc-modified-id=\"Preprocess-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Preprocess</a></span><ul class=\"toc-item\"><li><span><a href=\"#Top-domains\" data-toc-modified-id=\"Top-domains-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Top domains</a></span></li></ul></li><li><span><a href=\"#Try-Models\" data-toc-modified-id=\"Try-Models-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Try Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cosine\" data-toc-modified-id=\"Cosine-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Cosine</a></span></li><li><span><a href=\"#CatBoost\" data-toc-modified-id=\"CatBoost-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>CatBoost</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T15:39:07.459375Z",
     "start_time": "2020-10-10T15:39:07.452536Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T10:19:38.368517Z",
     "start_time": "2020-11-05T10:19:38.113346Z"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T10:19:39.636567Z",
     "start_time": "2020-11-05T10:19:38.618450Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import os, sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlretrieve, unquote\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain, product\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import unidecode\n",
    "from time import ctime\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T12:29:51.774779Z",
     "start_time": "2020-11-05T12:29:51.708065Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "tfidf_tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "stemmer = SnowballStemmer(\"russian\") \n",
    "stemmer2 = SnowballStemmer(\"english\") \n",
    "\n",
    "# Возвращает список словарей с url и timestamp из json \"url_data\"\n",
    "def get_timestamp_and_url(url_data):\n",
    "    return sorted(url_data[\"visits\"], key=lambda x: x[\"timestamp\"])\n",
    "\n",
    "# Возвращает разброс по времени посещений (в милисекундах) из json \"url_data\"\n",
    "def get_timestamp_range_from_url_data(url_data):\n",
    "    mi = np.inf\n",
    "    ma = -np.inf\n",
    "    for record in url_data[\"visits\"]:\n",
    "        mi = min(mi, record[\"timestamp\"])\n",
    "        ma = max(ma, record[\"timestamp\"])\n",
    "    return ma - mi\n",
    "\n",
    "# Преобразует строку с url в домен (с дополнительными символами b и ', например \"b'domain.com'\")\n",
    "def url2domain(url):\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    if containsAny(netloc, \":&@();= \"): return None\n",
    "    if netloc is not None: return str(netloc.encode('utf8')).strip()\n",
    "    return None\n",
    "\n",
    "def is_hex(s):\n",
    "    return re.fullmatch(r\"^[0-9a-fA-F]+$\", s or \"\") is not None\n",
    "\n",
    "def url2path_params_query(url):\n",
    "    url = unidecode.unidecode(url)\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: return None\n",
    "    return parsed_url.path\n",
    "\n",
    "def get_path_params_query(url_data, split_sessions=None, session_break=3600, mindf=0):\n",
    "    domains = []\n",
    "    domain_set = set()\n",
    "    if split_sessions is True:\n",
    "        sessions = []\n",
    "    prev_ts = 0\n",
    "    for record in sorted(url_data[\"visits\"], key=lambda x: x[\"timestamp\"]):\n",
    "        if split_sessions is True and prev_ts != 0:\n",
    "            if record[\"timestamp\"] > prev_ts + 1000*session_break:\n",
    "                sessions.append(\" \".join(domains))\n",
    "                domains = []\n",
    "        \n",
    "        prev_ts = record[\"timestamp\"]\n",
    "        domain = url2path_params_query(record[\"url\"])\n",
    "#         if domain not in domain_set:\n",
    "#             domain_set.add(domain)\n",
    "#         else:\n",
    "#             continue\n",
    "        domains.append(domain)\n",
    "    ##\n",
    "    splits = tfidf_tokenizer(\" \".join(domains))\n",
    "#     domains = {stemmer.stem(stemmer2.stem(i)) for i in splits if (len(i) > 3 and len(i) < 15 and str.isalpha(i))}\n",
    "    domains = {i for i in splits if (wcnt[i] > mindf and len(i) > 3 and len(i) < 15 and str.isalpha(i))}\n",
    "    ##\n",
    "    if split_sessions is True:\n",
    "        sessions.append(\" \".join(domains))\n",
    "        return sessions\n",
    "    return \" \".join(domains)\n",
    "\n",
    "# Возращает строку с доменами, разделенными пробелами, из json \"url_data\" (без дополнительных символов b и ')\n",
    "def get_domains(url_data, split_sessions=None, no_sequenses=False, session_break=3600, tf=False):\n",
    "##############    \n",
    "    hours = Counter()\n",
    "    weekdays = Counter()\n",
    "    weekends = Counter()\n",
    "    week_in_months = Counter()\n",
    "#     return pd.Series(np.hstack((hour_features, weekday_features, weekend_features, week_in_month_features)))\n",
    "##############\n",
    "    \n",
    "    domains = []\n",
    "    domain_set = set()\n",
    "    if split_sessions is True:\n",
    "        sessions = []\n",
    "    prev_ts = 0\n",
    "    prev_domain = None\n",
    "    for record in sorted(url_data[\"visits\"], key=lambda x: x[\"timestamp\"]):\n",
    "        if split_sessions is False and prev_ts != 0:\n",
    "            if record[\"timestamp\"] > prev_ts + 1000*session_break:\n",
    "                prev_domain = None\n",
    "        if split_sessions is True and prev_ts != 0:\n",
    "            if record[\"timestamp\"] > prev_ts + 1000*session_break:\n",
    "                sessions.append(\" \".join(domains))\n",
    "                domains = []\n",
    "                prev_domain = None\n",
    "            \n",
    "        prev_ts = record[\"timestamp\"]\n",
    "        tf_ = get_tf(prev_ts)\n",
    "        domain = url2domain(record[\"url\"])\n",
    "        \n",
    "        ###\n",
    "        if tf:\n",
    "            hours[tf_[\"hour\"]] += 1\n",
    "            weekdays[tf_[\"weekday\"]] += 1\n",
    "            weekends[tf_[\"weekend\"]] += 1\n",
    "            week_in_months[tf_[\"week_in_month\"]] += 1\n",
    "        ###\n",
    "        \n",
    "        if domain is None:\n",
    "            continue\n",
    "        domain = domain[2:-1]\n",
    "        dcnt[domain] += 1\n",
    "        if domain not in domain_set:\n",
    "            ducnt[domain] += 1\n",
    "            domain_set.add(domain)\n",
    "        wcnt.update(domain.split(\".\"))\n",
    "        \n",
    "        if no_sequenses and domain == prev_domain:\n",
    "            continue        \n",
    "        prev_domain = domain\n",
    "        domains.append(domain)\n",
    "#         ###\n",
    "#         if tf:\n",
    "#             hours[tf_[\"hour\"]] += 1\n",
    "#             weekdays[tf_[\"weekday\"]] += 1\n",
    "#             weekends[tf_[\"weekend\"]] += 1\n",
    "#             week_in_months[tf_[\"week_in_month\"]] += 1\n",
    "#         ###\n",
    "    if split_sessions is True:\n",
    "        sessions.append(\" \".join(domains))\n",
    "        return sessions\n",
    "    if tf:\n",
    "        hour_features = np.zeros(24, dtype=int)\n",
    "        weekday_features = np.zeros(7, dtype=int)\n",
    "        weekend_features = np.zeros(2, dtype=int)\n",
    "        week_in_month_features = np.zeros(5, dtype=int)\n",
    "        hour_features[list(hours.keys())] = list(hours.values())\n",
    "        weekday_features[list(weekdays.keys())] = list(weekdays.values())\n",
    "        weekend_features[list(weekends.keys())] = list(weekends.values())\n",
    "        week_in_month_features[list(week_in_months.keys())] = list(week_in_months.values())\n",
    "        \n",
    "        hour_features = hour_features / sum(hour_features)\n",
    "        weekday_features = weekday_features / sum(weekday_features)\n",
    "        weekend_features = weekend_features / sum(weekend_features)\n",
    "        week_in_month_features = week_in_month_features / sum(week_in_month_features)\n",
    "        \n",
    "        return pd.Series(np.hstack((\" \".join(domains), \n",
    "                                    hour_features, \n",
    "                                    weekday_features,\n",
    "                                    weekend_features, \n",
    "                                    week_in_month_features)))\n",
    "    else:\n",
    "        return pd.Series(\" \".join(domains))\n",
    "\n",
    "def get_tf(ts):\n",
    "    dt = datetime.fromtimestamp(ts/1000)\n",
    "    res = dict()\n",
    "    res[\"day\"] = dt.day\n",
    "    res[\"week_in_month\"] = (res[\"day\"] - 1) // 7\n",
    "    res[\"weekday\"] = dt.weekday()\n",
    "    res[\"weekend\"] = 1 if res[\"weekday\"] > 4 else 0\n",
    "    res[\"hour\"] = dt.hour\n",
    "    return res\n",
    "\n",
    "# Возвращает строку с доменами, разделенными пробелами, \n",
    "# полученную с помощью фильтрации аналогичной строки с фильтром top_domains\n",
    "def get_filtered_domains(url_data, top_domains, no_repeatition=False):\n",
    "    domains = []\n",
    "    domain_set = set()\n",
    "    for domain in url_data.split():\n",
    "        if domain in top_domains:\n",
    "            if not no_repeatition or domain not in domain_set:\n",
    "                domains.append(domain)\n",
    "                domain_set.add(domain)\n",
    "    return \" \".join(domains)\n",
    "\n",
    "# Возращает строку с полными url'ами, разделенными пробелами, из json \"url_data\"\n",
    "def get_urls(url_data):\n",
    "    urls = []\n",
    "    for record in url_data[\"visits\"]:\n",
    "        url = record[\"url\"]\n",
    "        urls.append(url)\n",
    "    return \" \".join(urls)\n",
    "\n",
    "# Возвращает столбец с доменами, с выбрасыванием из исходного столбца редко встречающихся доменов\n",
    "# domain_counts - DataFrame с информацией о посещаемости сайтов (см. Preprocess)\n",
    "# min_df - все домены, которые встречаются меньше min_df раз, будут исключены\n",
    "def get_top_domain_feature(df, domain_counts, min_df=0, no_repeatition=False):\n",
    "    top_domains = set(domain_counts[domain_counts.counts>min_df].index.values)\n",
    "    return df.domain.apply(get_filtered_domains, top_domains=top_domains, no_repeatition=no_repeatition)\n",
    "\n",
    "gm = {\"F\":0,\"M\":1}\n",
    "am = {\"18-24\":0, \"25-34\":1, \"35-44\":2, \"45-54\":3, \">=55\":4}\n",
    "def ga2cat(g, a):\n",
    "    return gm[g]*5+am[a]\n",
    "\n",
    "def cat2ga(c):\n",
    "    gg = [\"F\", \"M\"]\n",
    "    aa = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \">=55\"]\n",
    "    rg = gg[c//5]\n",
    "    if c > 4:\n",
    "        c -= 5\n",
    "    ra = aa[c]\n",
    "        \n",
    "    return rg, ra\n",
    "\n",
    "def containsAny(s, ch):\n",
    "    \"\"\" Check whether sequence str contains ANY of the items in set. \"\"\"\n",
    "    return 1 in [c in s for c in ch]\n",
    "\n",
    "def get_cosine_centr(vectorize_data, labels, features):\n",
    "# get cosine centr for vectorize_data\n",
    "# vectorize_data: pd.DataFrame\n",
    "    cosine_centr = {}\n",
    "    \n",
    "    for label_ in labels:\n",
    "        labels_val = vectorize_data[label_].unique()\n",
    "        for label__ in labels_val:\n",
    "            label__s = str(label__)\n",
    "            data_ = vectorize_data[vectorize_data[label_] == label__]\n",
    "            cosine_centr[label__s] = np.mean(data_[features]).values\n",
    "            cosine_centr[label__s] /= sqrt(sum(np.multiply(cosine_centr[label__s], cosine_centr[label__s])))\n",
    "#             if len(labels_val) > 2:\n",
    "#                 data_ = vectorize_data[vectorize_data[label_] != label__]\n",
    "#                 label__s = label__s+\"_\"\n",
    "#                 cosine_centr[label__s] = np.mean(data_[features]).values\n",
    "#                 cosine_centr[label__s] /= sqrt(sum(np.multiply(cosine_centr[label__s], cosine_centr[label__s])))\n",
    "    return cosine_centr\n",
    "\n",
    "def get_common_and_target(c1, c2):\n",
    "    common = (c1 + c2) / 2\n",
    "    common /= sqrt(sum(np.multiply(common, common)))\n",
    "    c1_ = c1 - common\n",
    "    c1_ /= sqrt(sum(np.multiply(c1_, c1_)))\n",
    "    c2_ = c2 - common\n",
    "    c2_ /= sqrt(sum(np.multiply(c2_, c2_)))\n",
    "    return common, c1_, c2_\n",
    "\n",
    "def get_common_and_target2(centers):\n",
    "    common = np.mean(centers, axis=0)\n",
    "    common /= sqrt(sum(np.multiply(common, common)))\n",
    "    nc = [c - common for c in centers]\n",
    "    nc = [c/sqrt(sum(np.multiply(c, c))) for c in nc]\n",
    "    return common, nc\n",
    "\n",
    "def fit_transform(df_mini, stop_words, domain_counts, min_df=10, no_repeatition=False,\n",
    "              split_sessions=False, no_sequenses=False, session_break=60*60*1, tf=True, centers=0):\n",
    "    tfidf_g = TfidfVectorizer(tokenizer=str.split, stop_words=stop_words)\n",
    "    tfidf_a = TfidfVectorizer(tokenizer=str.split)\n",
    "\n",
    "    time_features = []\n",
    "    if tf:\n",
    "        time_features =     [\"h_\"+str(i) for i in range(24)] + \\\n",
    "                            [\"wd_\"+str(i) for i in range(7)] + \\\n",
    "                            [\"we_\"+str(i) for i in range(2)] + \\\n",
    "                            [\"wm_\"+str(i) for i in range(5)]\n",
    "    folds = 5\n",
    "    gender_sim_cols = [\"fgs_f_\"+str(j) for j in range(folds)] +\\\n",
    "                      [\"fgs_m_\"+str(j) for j in range(folds)]\n",
    "    age_sim_cols = [\"fas_\"+str(i)+\"_\"+str(j) for i in range(5) for j in range(folds)] \n",
    "    label_cols = [\"gender\", \"age\"]\n",
    "    gender_label = \"g\"\n",
    "    age_label = \"a\"\n",
    "    cat_label = \"c\"\n",
    "\n",
    "    d_tf = df_mini[\"user_json\"].apply(json.loads).\\\n",
    "        apply(get_domains, split_sessions=split_sessions, no_sequenses=no_sequenses, session_break=session_break, tf=tf)\n",
    "    if tf:\n",
    "        cols = time_features\n",
    "        d_tf.columns = [\"domain\"] + cols\n",
    "    else:\n",
    "        cols = []\n",
    "        d_tf.columns = [\"domain\"]\n",
    "\n",
    "    d_tf[\"top_domain\"] = get_top_domain_feature(d_tf, domain_counts, min_df, no_repeatition=no_repeatition)\n",
    "\n",
    "    d_tf = df_mini[label_cols].join(d_tf[cols + [\"top_domain\"]])\n",
    "    d_tf.columns = label_cols + cols + ['domain']\n",
    "    d_tf[gender_label] = (d_tf.gender==\"M\").astype(int)\n",
    "    d_tf[age_label] = d_tf.age.map(am)\n",
    "    d_tf[cat_label] = d_tf.apply(lambda x: ga2cat(x['gender'], x['age']), axis=1)\n",
    "\n",
    "    features_g = tfidf_g.fit_transform(d_tf.domain)\n",
    "    features_a = tfidf_a.fit_transform(d_tf.domain)\n",
    "    tfidf_feats_g = [\"tfidf_g_\"+str(i) for i in range(features_g.shape[1])]\n",
    "    tfidf_feats_a = [\"tfidf_a_\"+str(i) for i in range(features_a.shape[1])]\n",
    "\n",
    "    d_tf = d_tf.join(\n",
    "        pd.DataFrame(data=features_g.toarray(), \n",
    "                     columns=tfidf_feats_g, \n",
    "                     index=d_tf.index)\n",
    "    ).join(\n",
    "        pd.DataFrame(data=features_a.toarray(), \n",
    "                     columns=tfidf_feats_a, \n",
    "                     index=d_tf.index)\n",
    "    )\n",
    "    \n",
    "\n",
    "    common_gender = [None]*folds\n",
    "    f_ct = [None]*folds\n",
    "    m_ct = [None]*folds\n",
    "    common_age = [None]*folds\n",
    "    a_ct = [None]*folds\n",
    "    common_cat = [None]*folds\n",
    "    c_ct = [None]*folds\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=folds, random_state=0, shuffle=True)\n",
    "    for j, (train, test) in enumerate(kf.split(d_tf, d_tf.c)):\n",
    "    \n",
    "        if centers==0:\n",
    "            subdf = d_tf.iloc[train]\n",
    "        else:\n",
    "            subdf = d_tf.iloc[test]\n",
    "            \n",
    "        centers_g = get_cosine_centr(subdf, [\"gender\"], tfidf_feats_g)\n",
    "        centers_a = get_cosine_centr(subdf, [\"age\"], tfidf_feats_a)\n",
    "        #\n",
    "        centers_c = get_cosine_centr(subdf, [\"c\"], tfidf_feats_a)\n",
    "        #\n",
    "\n",
    "        common_gender[j], f_ct[j], m_ct[j] = get_common_and_target(centers_g['F'], centers_g['M'])\n",
    "        common_age[j], a_ct[j] = get_common_and_target2([centers_a['18-24'],\n",
    "                                                   centers_a['25-34'],\n",
    "                                                   centers_a['35-44'],\n",
    "                                                   centers_a['45-54'],\n",
    "                                                   centers_a['>=55']])\n",
    "        #\n",
    "        common_cat[j], c_ct[j] = get_common_and_target2([centers_c[str(i)] for i in range(10)])\n",
    "        #\n",
    "\n",
    "        fg = d_tf[tfidf_feats_g] - common_gender[j]\n",
    "        fg = fg / sqrt(np.multiply(fg, fg).sum(axis=0))\n",
    "        fgs_f = cosine_similarity(f_ct[j].reshape(1, -1), fg)\n",
    "        fgs_m = cosine_similarity(m_ct[j].reshape(1, -1), fg)\n",
    "\n",
    "        fa = d_tf[tfidf_feats_a] - common_age[j]\n",
    "        fa = fa / sqrt(np.multiply(fa, fa).sum(axis=0))\n",
    "        n = 5\n",
    "        fas = [None]*n\n",
    "        for i in range(n):\n",
    "            fas[i] = cosine_similarity(a_ct[j][i].reshape(1, -1), fa)\n",
    "\n",
    "        #    \n",
    "        fc = d_tf[tfidf_feats_a] - common_cat[j]\n",
    "        fc = fc / sqrt(np.multiply(fc, fc).sum(axis=0))\n",
    "        fcs = [None]*10\n",
    "        for i in range(10):\n",
    "            fcs[i] = cosine_similarity(c_ct[j][i].reshape(1, -1), fc)\n",
    "        #\n",
    "\n",
    "        d_tf[\"fgs_f\"+\"_\"+str(j)] = fgs_f.T\n",
    "        d_tf[\"fgs_m\"+\"_\"+str(j)] = fgs_m.T\n",
    "        for i in range(n):\n",
    "            d_tf[\"fas_\"+str(i)+\"_\"+str(j)] = fas[i].T\n",
    "        #\n",
    "        for i in range(10):\n",
    "            d_tf[\"fcs_\"+str(i)+\"_\"+str(j)] = fcs[i].T\n",
    "        #\n",
    "    \n",
    "    gender_features = gender_sim_cols + tfidf_feats_g\n",
    "    age_features = age_sim_cols + tfidf_feats_a + time_features\n",
    "    cat_sim_cols = [\"fcs_\"+str(i)+\"_\"+str(j) for i in range(10) for j in range(folds)]\n",
    "    \n",
    "    return d_tf, gender_sim_cols, age_sim_cols, cat_sim_cols, time_features,\\\n",
    "                 tfidf_feats_g, tfidf_feats_a, tfidf_g, tfidf_a,\\\n",
    "                [common_gender, f_ct, m_ct], [common_age, a_ct], [common_cat, c_ct],\\\n",
    "                [label_cols, gender_label, age_label, cat_label]\n",
    "\n",
    "\n",
    "def transform(df_mini, stop_words, domain_counts, tfidf_g, tfidf_a, gender_ctr, age_ctr, cat_ctr,\n",
    "              min_df=10, no_repeatition=False,\n",
    "              split_sessions=False, no_sequenses=False, session_break=60*60*1, tf=True):\n",
    "    common_gender, f_ct, m_ct = gender_ctr\n",
    "    common_age, a_ct = age_ctr\n",
    "    common_cat, c_ct = cat_ctr\n",
    "    \n",
    "    time_features = []\n",
    "    if tf:\n",
    "        time_features =     [\"h_\"+str(i) for i in range(24)] + \\\n",
    "                            [\"wd_\"+str(i) for i in range(7)] + \\\n",
    "                            [\"we_\"+str(i) for i in range(2)] + \\\n",
    "                            [\"wm_\"+str(i) for i in range(5)]\n",
    "        \n",
    "    folds = 5\n",
    "    gender_sim_cols = [\"fgs_f\", \"fgs_m\"]\n",
    "    age_sim_cols = [\"fas_\"+str(i) for i in range(5)]\n",
    "    cat_sim_cols = [\"fcs_\"+str(i)+\"_\"+str(j) for i in range(10) for j in range(folds)]\n",
    "\n",
    "    d_tf = df_mini[\"user_json\"].apply(json.loads).\\\n",
    "        apply(get_domains, split_sessions=split_sessions, no_sequenses=no_sequenses, session_break=session_break, tf=tf)\n",
    "    if tf:\n",
    "        cols = time_features\n",
    "        d_tf.columns = [\"domain\"] + cols\n",
    "    else:\n",
    "        cols = []\n",
    "        d_tf.columns = [\"domain\"]\n",
    "\n",
    "    d_tf[\"top_domain\"] = get_top_domain_feature(d_tf, domain_counts, min_df, no_repeatition=no_repeatition)\n",
    "\n",
    "    d_tf = df_mini[label_cols].join(d_tf[cols + [\"top_domain\"]])\n",
    "    d_tf.columns = label_cols + cols + ['domain']\n",
    "\n",
    "    features_g = tfidf_g.transform(d_tf.domain)\n",
    "    features_a = tfidf_a.transform(d_tf.domain)\n",
    "    tfidf_feats_g = [\"tfidf_g_\"+str(i) for i in range(features_g.shape[1])]\n",
    "    tfidf_feats_a = [\"tfidf_a_\"+str(i) for i in range(features_a.shape[1])]\n",
    "\n",
    "    d_tf = d_tf.join(\n",
    "        pd.DataFrame(data=features_g.toarray(), \n",
    "                     columns=tfidf_feats_g, \n",
    "                     index=d_tf.index)\n",
    "    ).join(\n",
    "        pd.DataFrame(data=features_a.toarray(), \n",
    "                     columns=tfidf_feats_a, \n",
    "                     index=d_tf.index)\n",
    "    )\n",
    "\n",
    "    for j in range(5):\n",
    "        fg = d_tf[tfidf_feats_g] - common_gender[j]\n",
    "        fg = fg / sqrt(np.multiply(fg, fg).sum(axis=0))\n",
    "        fgs_f = cosine_similarity(f_ct[j].reshape(1, -1), fg)\n",
    "        fgs_m = cosine_similarity(m_ct[j].reshape(1, -1), fg)\n",
    "\n",
    "        fa = d_tf[tfidf_feats_a] - common_age[j]\n",
    "        fa = fa / sqrt(np.multiply(fa, fa).sum(axis=0))\n",
    "        n = 5\n",
    "        fas = [None]*n\n",
    "        for i in range(n):\n",
    "            fas[i] = cosine_similarity(a_ct[j][i].reshape(1, -1), fa)\n",
    "\n",
    "        #    \n",
    "        fc = d_tf[tfidf_feats_a] - common_cat[j]\n",
    "        fc = fc / sqrt(np.multiply(fc, fc).sum(axis=0))\n",
    "        fcs = [None]*10\n",
    "        for i in range(10):\n",
    "            fcs[i] = cosine_similarity(c_ct[j][i].reshape(1, -1), fc)\n",
    "        #\n",
    "\n",
    "        d_tf[\"fgs_f\"+\"_\"+str(j)] = fgs_f.T\n",
    "        d_tf[\"fgs_m\"+\"_\"+str(j)] = fgs_m.T\n",
    "        for i in range(n):\n",
    "            d_tf[\"fas_\"+str(i)+\"_\"+str(j)] = fas[i].T\n",
    "        #\n",
    "        for i in range(10):\n",
    "            d_tf[\"fcs_\"+str(i)+\"_\"+str(j)] = fcs[i].T\n",
    "        #\n",
    "    \n",
    "#     gender_features = gender_sim_cols + tfidf_feats_g\n",
    "#     age_features = age_sim_cols + tfidf_feats_a + time_features\n",
    "    \n",
    "    return d_tf#, gender_features, age_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T19:53:02.919216Z",
     "start_time": "2020-10-30T19:53:02.910567Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_time(url_data):\n",
    "# get features: how many times for that hour, weekday, weekend, ... user visited urls\n",
    "    \n",
    "    hour_features = np.zeros(24, dtype=int)\n",
    "    weekday_features = np.zeros(7, dtype=int)\n",
    "    weekend_features = np.zeros(2, dtype=int)\n",
    "    week_in_month_features = np.zeros(5, dtype=int)\n",
    "    \n",
    "    data_ = pd.DataFrame(url_data[\"visits\"]).copy()\n",
    "    data_[\"timestamp\"] = data_[\"timestamp\"] / 1000\n",
    "    data_[\"timestamp\"] = data_.apply(lambda x: datetime.fromtimestamp(x[\"timestamp\"] ), axis=1)\n",
    "    data_[\"day\"] = data_.apply(lambda x: x[\"timestamp\"].day, axis=1)\n",
    "    data_[\"week_in_month\"] = (data_.day - 1) // 7 \n",
    "    data_[\"weekday\"] = data_.apply(lambda x: x[\"timestamp\"].weekday(), axis=1)\n",
    "    data_[\"weekend\"] = data_.apply(lambda x: 1 if x.weekday > 4 else 0, axis=1)\n",
    "    data_[\"hour\"] = data_.apply(lambda x: x[\"timestamp\"].hour, axis=1)    \n",
    "    \n",
    "    hours = Counter(data_.hour)\n",
    "    hour_features[list(hours.keys())] = list(hours.values())\n",
    "    \n",
    "    weekdays = Counter(data_.weekday)\n",
    "    weekday_features[list(weekdays.keys())] = list(weekdays.values())\n",
    "    \n",
    "    weekends = Counter(data_.weekend)\n",
    "    weekend_features[list(weekends.keys())] = list(weekends.values())\n",
    "    \n",
    "    week_in_months = Counter(data_.week_in_month)\n",
    "    week_in_month_features[list(week_in_months.keys())] = list(week_in_months.values())\n",
    "    \n",
    "    return pd.Series(np.hstack((hour_features, weekday_features, weekend_features, week_in_month_features)))\n",
    "\n",
    "# test_dd =  pd.DataFrame(df[ : 2][\"user_json\"].apply(json.loads)).copy()\n",
    "# test_dd[\"user_json\"].apply(get_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T12:33:00.613318Z",
     "start_time": "2020-10-11T12:33:00.562727Z"
    }
   },
   "outputs": [],
   "source": [
    "# example = {\"visits\": [{'url': 'http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun', 'timestamp': 1419688144068}, {'url': 'http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story', 'timestamp': 1426666298001}, {'url': 'http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html', 'timestamp': 1426666298000}, {'url': 'http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story', 'timestamp': 1426661722001}, {'url': 'http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html', 'timestamp': 1426661722000}]}\n",
    "# print('\"'+get_domains(example)+'\"')\n",
    "# print('\"'+get_urls(example)+'\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T19:15:29.436258Z",
     "start_time": "2020-10-30T19:15:18.877591Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/data/share/project01/gender_age_dataset.txt'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "df.shape\n",
    "is_processed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T13:40:49.905702Z",
     "start_time": "2020-10-28T13:40:14.236548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Считает число встречающихся доменов, учитывает домен несколько раз, если встретился несколько раз у одного пользователя\n",
    "dcnt = Counter()   \n",
    "# Считает число встречающихся доменов, учитывает домен один раз, если встретился несколько раз у одного пользователя\n",
    "ducnt = Counter()  \n",
    "# Считает число встречающихся \"слов\"\n",
    "wcnt = Counter() \n",
    "\n",
    "# Достаем домены из json-строки. Как побочный эффект функция get_domains наполняет счетчики\n",
    "df[\"domain\"] = df[\"user_json\"].\\\n",
    "        apply(json.loads).\\\n",
    "        apply(get_domains) # vw.txt\n",
    "#         apply(get_domains, split_sessions=False, no_sequenses=True) # vw2.txt\n",
    "\n",
    "# Топ доменов\n",
    "domain_counts = pd.DataFrame(ducnt.most_common()).set_index(0)\n",
    "domain_counts.index.name = \"domain\"\n",
    "domain_counts.columns = [\"counts\"]\n",
    "# domain_counts.head()\n",
    "\n",
    "is_processed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T17:25:48.803511Z",
     "start_time": "2020-10-25T17:24:03.603897Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"words\"] = df[\"user_json\"].\\\n",
    "        apply(json.loads).\\\n",
    "        apply(get_path_params_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T13:40:49.909078Z",
     "start_time": "2020-10-28T13:39:56.039Z"
    }
   },
   "outputs": [],
   "source": [
    "known = df[~((df.gender == '-') & (df.age == '-'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T17:26:08.236069Z",
     "start_time": "2020-10-25T17:26:03.921404Z"
    }
   },
   "outputs": [],
   "source": [
    "known[\"age_\"] = known.age.map(am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T17:26:19.885058Z",
     "start_time": "2020-10-25T17:26:18.364033Z"
    }
   },
   "outputs": [],
   "source": [
    "known[\"domain\"] = known[[\"domain\", \"words\"]].apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T14:53:40.688093Z",
     "start_time": "2020-10-25T14:53:29.902639Z"
    }
   },
   "outputs": [],
   "source": [
    "# # save\n",
    "# if is_processed:\n",
    "df.to_pickle(\"data/df_new.pkl\")\n",
    "#     domain_counts.to_pickle(\"data/dc.pkl\")\n",
    "# with open(\"dcnt.pkl\", \"wb\") as dcnt_pkl:\n",
    "#     pickle.dump([dcnt, ducnt, wcnt], dcnt_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T13:46:00.972581Z",
     "start_time": "2020-10-18T13:46:00.364204Z"
    }
   },
   "outputs": [],
   "source": [
    "# # save\n",
    "# if is_processed:\n",
    "#     df.to_pickle(\"data/df.pkl\")\n",
    "#     domain_counts.to_pickle(\"data/dc.pkl\")\n",
    "# with open(\"dcnt.pkl\", \"wb\") as dcnt_pkl:\n",
    "#     pickle.dump([dcnt, ducnt, wcnt], dcnt_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T08:08:42.510494Z",
     "start_time": "2020-10-21T08:08:38.387442Z"
    }
   },
   "outputs": [],
   "source": [
    "# # load\n",
    "# df = pd.read_pickle(\"data/df.pkl\")\n",
    "# domain_counts = pd.read_pickle(\"data/dc.pkl\")\n",
    "# with open(\"dcnt.pkl\", \"rb\") as dcnt_pkl:\n",
    "#     dcnt, ducnt, wcnt = pickle.load(dcnt_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T12:47:52.768338Z",
     "start_time": "2020-10-18T12:47:38.189158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Столбец с выброшенными редкими доменами\n",
    "df[\"top_domain\"] = get_top_domain_feature(df, domain_counts, 10, no_repeatition=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T19:21:32.737068Z",
     "start_time": "2020-10-11T19:21:32.705591Z"
    }
   },
   "outputs": [],
   "source": [
    "df.top_domain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T19:21:39.583748Z",
     "start_time": "2020-10-11T19:21:39.578741Z"
    }
   },
   "outputs": [],
   "source": [
    "df.domain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T19:23:25.667300Z",
     "start_time": "2020-10-11T19:23:11.907616Z"
    }
   },
   "outputs": [],
   "source": [
    "time_range = df[\"user_json\"].\\\n",
    "        apply(json.loads).\\\n",
    "        apply(get_timestamp_range_from_url_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T19:23:32.472706Z",
     "start_time": "2020-10-11T19:23:32.456553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Значения в днях\n",
    "(time_range/1000/60/60/24).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T19:23:40.179301Z",
     "start_time": "2020-10-11T19:23:39.583360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Число посещений (всех)\n",
    "visits = df.domain.apply(lambda x: len(str.split(x)))\n",
    "visits.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T19:23:47.016083Z",
     "start_time": "2020-10-11T19:23:46.912711Z"
    }
   },
   "outputs": [],
   "source": [
    "# Число посещений (без редких 10, без повторных заходов)\n",
    "top_visits = df.top_domain.apply(lambda x: len(str.split(x)))\n",
    "top_visits.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T10:20:17.777727Z",
     "start_time": "2020-11-05T10:20:14.895668Z"
    }
   },
   "outputs": [],
   "source": [
    "# load\n",
    "df = pd.read_pickle(\"data/df.pkl\")\n",
    "domain_counts = pd.read_pickle(\"data/dc.pkl\")\n",
    "with open(\"dcnt.pkl\", \"rb\") as dcnt_pkl:\n",
    "    dcnt, ducnt, wcnt = pickle.load(dcnt_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T11:09:32.209013Z",
     "start_time": "2020-11-05T11:09:30.481402Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "stop_words = ['cache.betweendigital.com', 'google.am', 'google.az', 'google.by', 'google.de', 'google.fi', 'google.ge', 'google.it', 'google.kz', 'google.lv', 'google.md', 'google.ru', 'google.com', 'google.co.il', 'google.com.ua', 'googleadservices.com', 'tpc.googlesyndication.com', 'pagead2.googlesyndication.com', 'webcache.googleusercontent.com', 'translate.googleusercontent.com', 'rambler.ru', 'id.rambler.ru', 'help.rambler.ru', 'mail.rambler.ru', 'vk.com', 'm.vk.com', 'api.vk.com', 'playvk.com', 'kinogo.net', 'kinogo-net.ru', 'ozon.ru', 'adme.ru', 'ololo.fm', 'hhcdn.ru', 'gismeteo.by', 'gismeteo.md', 'gismeteo.ru', 'gismeteo.ua', 'beta.gismeteo.ru', 'weather.com', 'accuweather.com', 'world-weather.ru', 'm.accuweather.com', 'weather.rambler.ru', 'weatherandtime.net', 'dic.academic.ru', 'films.imhonet.ru', 'rzd.ru', 'doc.rzd.ru', 'kbsh.rzd.ru', 'pass.rzd.ru', 'cargo.rzd.ru', 'press.rzd.ru', 'young.rzd.ru', 'social.rzd.ru', 'contacts.rzd.ru', 'fb.ru', 'm.fb.ru', 'facebook.com', 'l.facebook.com', 'm.facebook.com', 'lm.facebook.com', 'app.facebook.com', 'apps.facebook.com', 'litres.ru', 'muzofon.com', 'ok.ru', 'lk.ssl.mts.ru', 'pay.mts.ru', 'spb.mts.ru', 'shop.mts.ru', 'bonus.mts.ru', 'login.mts.ru', 'oauth.mts.ru', 'goodok.mts.ru', 'legacy.mts.ru', 'lk.ssl.mts.ru', 'ihelper.mts.ru', 'kabinet.mts.ru', 'ihelper.nw.mts.ru', 'ihelper.sib.mts.ru', 'ihelper.nnov.mts.ru', 'irr.ru', 'm.irr.ru', 'ufa.irr.ru', 'omsk.irr.ru', 'orel.irr.ru', 'perm.irr.ru', 'tula.irr.ru', 'tver.irr.ru', 'kazan.irr.ru', 'kirov.irr.ru', 'sochi.irr.ru', 'tomsk.irr.ru', 'russia.irr.ru', 'ryazan.irr.ru', 'samara.irr.ru', 'tambov.irr.ru', 'tyumen.irr.ru', 'barnaul.irr.ru', 'bryansk.irr.ru', 'irkutsk.irr.ru', 'ivanovo.irr.ru', 'izhevsk.irr.ru', 'lipetsk.irr.ru', 'obninsk.irr.ru', 'saratov.irr.ru', 'belgorod.irr.ru', 'kostroma.irr.ru', 'orenburg.irr.ru', 'tolyatti.irr.ru', 'vladimir.irr.ru', 'voronezh.irr.ru', 'krasnodar.irr.ru', 'nabchelny.irr.ru', 'ulyanovsk.irr.ru', 'volgograd.irr.ru', 'yaroslavl.irr.ru', 'khabarovsk.irr.ru', 'chelyabinsk.irr.ru', 'kaliningrad.irr.ru', 'krasnoyarsk.irr.ru', 'novosibirsk.irr.ru', 'velnovgorod.irr.ru', 'ekaterinburg.irr.ru', 'magnitogorsk.irr.ru', 'novokuznetsk.irr.ru', 'petrozavodsk.irr.ru', 'rostovnadonu.irr.ru', 'tulskaya-obl.irr.ru', 'nizhniynovgorod.irr.ru', 'saint-petersburg.irr.ru', 'vladimirskaya-obl.irr.ru', 'krasnodarskiy-kray.irr.ru', 'nizhegorodskaya-obl.irr.ru', '24open.ru', 'adme.ru', 'bigcinema.tv', 'bolshoyvopros.ru', 'cache.betweendigital.com', 'dic.academic.ru', 'enter.ru', 'fast-torrent.ru', 'films.imhonet.ru', 'gismeteo.ru', 'beta.gismeteo.ru', 'go.mail.ru', 'google.com.ua', 'google.ru', 'hhcdn.ru', 'kinogo.net', 'kinogo-net.ru', 'mail.rambler.ru', 'muzofon.com', 'ololo.fm', 'ozon.ru', 'pass.rzd.ru', 'rambler.ru', 'id.rambler.ru', 'help.rambler.ru', 'mail.rambler.ru', 'seasonvar.ru', 'vk.com', 'm.vk.com', 'api.vk.com', 'playvk.com', 'wow-impulse.ru', 'sp.wow-impulse.ru', 'yandex.ru', 'an.yandex.ru', 'tv.yandex.ru', 'yandex.ua', 'docs.google.com', 'news.google.com.ua', 'nova.rambler.ru', 'weather.rambler.ru', 'reebok.ru', '24medok.ru', 'clck.yandex.ru', 'mail.yandex.ru', 'maps.yandex.ru', 'music.yandex.ru', 'pogoda.yandex.ru', 'rabota.yandex.ru','news.google.com.ua', 'nova.rambler.ru', 'maps.yandex.ru', '\\\\xd1\\\\x81\\\\xd0\\\\xbe\\\\xd1\\\\x84\\\\xd1\\\\x82-\\\\xd0\\\\xb2\\\\xd0\\\\xb0\\\\xd1\\\\x80\\\\xd0\\\\xb5\\\\xd0\\\\xb7.\\\\xd1\\\\x80\\\\xd1\\\\x84', '\\\\xd0\\\\xb0\\\\xd0\\\\xbd\\\\xd0\\\\xb0\\\\xd0\\\\xbb\\\\xd0\\\\xbe\\\\xd0\\\\xb3\\\\xd0\\\\xb8-\\\\xd0\\\\xb4\\\\xd0\\\\xbe\\\\xd1\\\\x80\\\\xd0\\\\xbe\\\\xd0\\\\xb3\\\\xd0\\\\xb8\\\\xd1\\\\x85-\\\\xd0\\\\xbb\\\\xd0\\\\xb5\\\\xd0\\\\xba\\\\xd0\\\\xb0\\\\xd1\\\\x80\\\\xd1\\\\x81\\\\xd1\\\\x82\\\\xd0\\\\xb2.\\\\xd1\\\\x80\\\\xd1\\\\x84']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:24:30.445967Z",
     "start_time": "2020-11-05T13:08:48.136379Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mini = df.copy()\n",
    "df_mini = df_mini[~((df_mini.gender == '-') & (df_mini.age == '-'))]\n",
    "\n",
    "d_tf, gender_sim_cols, age_sim_cols, cat_sim_cols, time_features,\\\n",
    "                 tfidf_feats_g, tfidf_feats_a, tfidf_g, tfidf_a,\\\n",
    "                [common_gender, f_ct, m_ct], [common_age, a_ct], [common_cat, c_ct],\\\n",
    "                [label_cols, gender_label, age_label, cat_label] =\\\n",
    "\\\n",
    "    fit_transform(df_mini, stop_words, domain_counts, no_repeatition=True, no_sequenses=True, tf=True, centers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:25:29.598346Z",
     "start_time": "2020-11-05T13:24:35.184283Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump((d_tf, gender_sim_cols, age_sim_cols, cat_sim_cols, time_features,\\\n",
    "                 tfidf_feats_g, tfidf_feats_a, tfidf_g, tfidf_a,\\\n",
    "                [common_gender, f_ct, m_ct], [common_age, a_ct], [common_cat, c_ct],\\\n",
    "                [label_cols, gender_label, age_label, cat_label]), open(\"new_tr8.pkl\", 'wb'), protocol=4)\n",
    "!rm new_tr8.zip\n",
    "!zip new_tr8.zip new_tr8.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:25:34.272984Z",
     "start_time": "2020-11-05T13:25:34.201822Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump((gender_sim_cols, age_sim_cols, cat_sim_cols, time_features,\\\n",
    "                 tfidf_feats_g, tfidf_feats_a, tfidf_g, tfidf_a,\\\n",
    "                [common_gender, f_ct, m_ct], [common_age, a_ct], [common_cat, c_ct],\\\n",
    "                [label_cols, gender_label, age_label, cat_label]), open(\"proj8.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:25:38.712737Z",
     "start_time": "2020-11-05T13:25:38.709703Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(\"proj.pkl\", \"rb\") as f:\n",
    "#     gender_sim_cols, age_sim_cols, cat_sim_cols, time_features, \\\n",
    "#     tfidf_feats_g, tfidf_feats_a, tfidf_g, tfidf_a, \\\n",
    "#     gender_ctr, age_ctr, cat_ctr, \\\n",
    "#     [label_cols, gender_label, age_label, cat_label] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:25:43.034521Z",
     "start_time": "2020-11-05T13:25:43.031781Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(tfidf_feats_g), len(tfidf_feats_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:25:47.355132Z",
     "start_time": "2020-11-05T13:25:47.350399Z"
    }
   },
   "outputs": [],
   "source": [
    "gender_ctr = [common_gender, f_ct, m_ct]\n",
    "age_ctr = [common_age, a_ct]\n",
    "cat_ctr = [common_cat, c_ct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:42:30.404159Z",
     "start_time": "2020-11-05T13:25:51.517353Z"
    }
   },
   "outputs": [],
   "source": [
    "ddd = df_mini.copy()\n",
    "ddd = transform(ddd, stop_words, domain_counts, tfidf_g, tfidf_a, gender_ctr, age_ctr, cat_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:42:38.832322Z",
     "start_time": "2020-11-05T13:42:38.264474Z"
    }
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "pylab.step(range(gender_ctr[1][j].shape[0]), gender_ctr[1][j])\n",
    "pylab.step(range(gender_ctr[2][j].shape[0]), gender_ctr[2][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:48:16.120697Z",
     "start_time": "2020-11-05T13:48:15.618716Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sn.distplot(ddd[\"fgs_f_0\"], hist=False, label=\"f\")\n",
    "sn.distplot(ddd[\"fgs_m_0\"], hist=False, label=\"m\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:47:24.392031Z",
     "start_time": "2020-11-05T13:47:24.075455Z"
    }
   },
   "outputs": [],
   "source": [
    "sn.distplot(ddd[\"fgs_f_0\"][d_tf[gender_label]==0], hist=False, label=\"f\")\n",
    "sn.distplot(ddd[\"fgs_f_0\"][d_tf[gender_label]==1], hist=False, label=\"m\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:48:00.977560Z",
     "start_time": "2020-11-05T13:48:00.518328Z"
    }
   },
   "outputs": [],
   "source": [
    "sn.distplot(ddd[\"fgs_m_0\"][d_tf[gender_label]==0], hist=False, label=\"f\")\n",
    "sn.distplot(ddd[\"fgs_m_0\"][d_tf[gender_label]==1], hist=False, label=\"m\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T21:41:27.648044Z",
     "start_time": "2020-10-28T21:41:24.248645Z"
    }
   },
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T10:24:46.792637Z",
     "start_time": "2020-10-31T10:24:46.775824Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    loss_function='MultiClass',\n",
    "#     loss_function='Logloss',\n",
    "    eval_metric='Accuracy',\n",
    "    random_seed=42,\n",
    "    logging_level='Silent',\n",
    "    learning_rate=0.05,\n",
    "    rsm=0.6,\n",
    "    subsample=0.6,\n",
    "    max_depth=7,\n",
    "    n_estimators=100,\n",
    "    bootstrap_type='Bernoulli',\n",
    "    \n",
    "    od_pval=1,\n",
    "    od_wait=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T10:32:19.107078Z",
     "start_time": "2020-10-31T10:24:54.583983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_[cols], y_cat_train,\n",
    "    eval_set=(X_test_[cols], y_cat_test),\n",
    "    logging_level='Verbose',  # you can uncomment this for text output\n",
    "#     plot=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:19:27.947287Z",
     "start_time": "2020-10-31T00:18:54.499826Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict_proba(X_test_[cols])\n",
    "test_preds_proba = preds.max(axis=1)\n",
    "test_preds = preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:21:15.346492Z",
     "start_time": "2020-10-31T00:19:31.158205Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict_proba(X_train_[cols])\n",
    "train_preds_proba = preds.max(axis=1)\n",
    "train_preds = preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:21:18.403919Z",
     "start_time": "2020-10-31T00:21:18.398055Z"
    }
   },
   "outputs": [],
   "source": [
    "train_good = train_preds_proba.argsort()[::-1][:train_preds_proba.shape[0]//2+1]\n",
    "test_good = test_preds_proba.argsort()[::-1][:test_preds_proba.shape[0]//2+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:21:21.128610Z",
     "start_time": "2020-10-31T00:21:21.121979Z"
    }
   },
   "outputs": [],
   "source": [
    "sum((test_preds == y_cat_test).iloc[test_good]) / test_good.shape[0], test_good.shape[0] / y_cat_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:21:23.862982Z",
     "start_time": "2020-10-31T00:21:23.849574Z"
    }
   },
   "outputs": [],
   "source": [
    "sum((train_preds == y_cat_train).iloc[train_good]) / train_good.shape[0], train_good.shape[0] / y_cat_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:21:45.681139Z",
     "start_time": "2020-10-31T00:21:45.618793Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_feature_importance(prettified=True) "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
