{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаба 5: Определение принадлежности текстов к заданной тематике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-zА-Яа-яёЁ]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention - generator\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for i in tokens:\n",
    "        if len(i)>2:\n",
    "            yield m.lemmatize(i)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные из ЛК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids of texts you need to classify\n",
    "ids=[2561, 514, 2570, 3085, 1040, 1046, 24, 3610, 3612, 859, 33, 945, 2087, 1885, 1580, 1074, 3639, 1593, 2485, 573, 63, 3649, 3139, 582, 2049, 1608, 1098, 1099, 2639, 3668, 1626, 607, 2656, 1121, 1635, 3178, 1644, 2158, 2159, 2238, 3699, 3700, 2682, 2173, 2174, 3203, 2692, 2184, 2187, 536, 3213, 3214, 3728, 3729, 658, 2195, 3222, 2199, 2200, 624, 672, 161, 1698, 3748, 711, 3239, 3244, 1710, 176, 3763, 1396, 1717, 695, 698, 1723, 1725, 702, 1215, 1217, 1320, 3267, 196, 710, 3783, 3272, 715, 2255, 724, 1758, 3808, 3300, 3818, 748, 3822, 751, 2288, 3180, 3317, 2806, 3831, 384, 623, 3030, 3842, 3850, 2316, 783, 2324, 1305, 2332, 1311, 1825, 292, 1831, 562, 812, 2863, 3376, 307, 820, 3551, 2361, 833, 2374, 2377, 1355, 3920, 337, 2386, 1367, 1308, 347, 861, 352, 3473, 3940, 870, 2921, 2924, 1524, 13, 2758, 372, 1909, 887, 888, 382, 1174, 897, 3137, 903, 1419, 579, 1006, 1937, 917, 406, 2456, 414, 2463, 3489, 668, 2981, 429, 2760, 1457, 3508, 3509, 1439, 2634, 3001, 3516, 445, 2379, 2496, 1697, 971, 972, 2510, 464, 2003, 1956, 3542, 2521, 2010, 1500, 2526, 479, 3041, 482, 2532, 485, 2023, 3564, 3566, 766, 2548, 3576, 3066, 1020]\n",
    "texts_index = list(zip(range(200), ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids), len(texts_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(1,21):\n",
    "    #f = codecs.open('/data/lab05data/data/base_'+str(i)+'.txt', 'r', 'utf-8')\n",
    "    f = open('/data/share/lab05data/base_'+str(i)+'.txt', 'r', encoding='utf-8')\n",
    "    text = f.readlines()\n",
    "    texts.append(BeautifulSoup(text[0], \"lxml\").text.lower())\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ids:\n",
    "    #f = codecs.open('/data/lab05data/data/test_'+str(i)+'.txt', 'r', 'utf-8')\n",
    "    f = open('/data/share/lab05data/test_'+str(i)+'.txt', 'r', encoding='utf-8')\n",
    "    text = f.readlines()\n",
    "    texts.append(BeautifulSoup(text[0], \"lxml\").text.lower())\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(texts[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем собственный токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-zА-Яа-яёЁ]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention - generator\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for i in tokens:\n",
    "        if len(i)>2:\n",
    "            yield m.lemmatize(i)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"еще\", \"него\", \"сказать\", \"а\", \"ж\", \"нее\", \"со\", \"без\", \"же\", \"ней\", \"совсем\", \"более\", \"жизнь\", \n",
    "              \"нельзя\", \"так\", \"больше\", \"за\", \"нет\", \"такой\", \"будет\", \"зачем\", \"ни\", \"там\", \"будто\", \"здесь\", \n",
    "              \"нибудь\", \"тебя\", \"бы\", \"и\", \"никогда\", \"тем\", \"был\", \"из\", \"ним\", \"теперь\", \"была\", \"из\", \"за\",\n",
    "              \"них\", \"то\", \"были\", \"или\", \"ничего\", \"тогда\", \"было\", \"им\", \"но\", \"того\", \"быть\", \"иногда\", \"ну\", \n",
    "              \"тоже\", \"в\", \"их\", \"о\", \"только\", \"вам\", \"к\", \"об\", \"том\", \"вас\", \"кажется\", \"один\", \"тот\", \"вдруг\",\n",
    "              \"как\", \"он\", \"три\", \"ведь\", \"какая\", \"она\", \"тут\", \"во\", \"какой\", \"они\", \"ты\", \"вот\", \"когда\", \"опять\",\n",
    "              \"у\", \"впрочем\", \"конечно\", \"от\", \"уж\", \"все\", \"которого\", \"перед\", \"уже\", \"всегда\", \"которые\", \"по\",\n",
    "              \"хорошо\", \"всего\", \"кто\", \"под\", \"хоть\", \"всех\", \"куда\", \"после\", \"чего\", \"всю\", \"ли\", \"потом\", \"человек\",\n",
    "              \"вы\", \"лучше\", \"потому\", \"чем\", \"г\", \"между\", \"почти\", \"через\", \"где\", \"меня\", \"при\", \"что\", \"говорил\",\n",
    "              \"мне\", \"про\", \"чтоб\", \"да\", \"много\", \"раз\", \"чтобы\", \"даже\", \"может\", \"разве\", \"чуть\", \"два\", \"можно\",\n",
    "              \"с\", \"эти\", \"для\", \"мой\", \"сам\", \"этого\", \"до\", \"моя\", \"свое\", \"этой\", \"другой\", \"мы\", \"свою\", \"этом\",\n",
    "              \"его\", \"на\", \"себе\", \"этот\", \"ее\", \"над\", \"себя\", \"эту\", \"ей\", \"надо\", \"сегодня\", \"я\", \"ему\", \"наконец\",\n",
    "              \"сейчас\", \"если\", \"нас\", \"сказал\", \"есть\", \"не\", \"сказала\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF для всех тесктов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(norm=None, \n",
    "                            smooth_idf=False, \n",
    "                            token_pattern='[A-Za-zА-Яа-яёЁ]+', \n",
    "                            stop_words=stop_words)\n",
    "\n",
    "m_tfidf = tfidf_vec.fit_transform(texts)\n",
    "matrix_tfidf = m_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matrix_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "d = pairwise_distances(matrix_tfidf, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Так как текстов заданной тематики несколько, результирующую метрику на неизвестный текст можно получить путем сложения косинусной меры этого текста со всеми эталонными текстами. При этом сумма косинусных мер может оказаться больше единицы.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d20 = 1 - abs(d[:20][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=list(d20.sum(axis=0)[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sorted(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#С токенайзером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(norm=None, \n",
    "                            smooth_idf=False, \n",
    "                            tokenizer=tokenize, \n",
    "                            stop_words=stop_words)\n",
    "matrix_tfidf = tfidf_vec.fit_transform(texts).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d = pairwise_distances(matrix_tfidf, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d20 = 1 - abs(d[:20][:])\n",
    "y=list(d20.sum(axis=0)[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sorted(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))\n",
    "print(sum(y))\n",
    "print(matrix_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = sum(y)/200\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = {\n",
    "    'defined': [],\n",
    "    'other': []\n",
    "}\n",
    "\n",
    "for i in y:\n",
    "    if i > average:\n",
    "        result['defined'].append(texts_index[y.index(i)][1])\n",
    "    else:\n",
    "        result['other'].append(texts_index[y.index(i)][1])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Выходной файл должен быть расположен в корне вашей директории в файле lab05.json. Чекер будет брать файл именно оттуда.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"lab05.json\", \"w\") as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лабораторная будет считаться пройденной, если метрики:\n",
    "\n",
    "len(TrueDefined.intersection(YourDefined))/len(YourDefined)\n",
    "\n",
    "len(TrueDefined.intersection(YourDefined))/len(TrueDefined)\n",
    "\n",
    "будут больше 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "time: 2019-20-04 11:56\n",
    "result is correct: True\n",
    "len(TrueDefined.intersection(YourDefined))/len(TrueDefined): 1.0\n",
    "len(TrueDefined.intersection(YourDefined))/len(YourDefined): 1.0\n",
    "checker has successfully read all data in file: True\n",
    "file lab05.json exists in your dir: True\n",
    "file has size less than 20 000 bytes: True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
